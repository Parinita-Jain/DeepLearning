Lecture1---https://drive.google.com/drive/folders/1NT4ZX_o2JS80gAhQiM276l224ldkQRN9

D/B ML,Deep Learning and AI

AI termis coined in year 1956

Ai -: Aritificial Intelligence -: 1956  -: Mimic the Human Behaviour
ML -: Machine Learning 		-: 1959 -: teach our system -: Learn -: 
Deep learning -: Deep Learning	-: 2000 -: Mimic the Human Brain -: 
Aritificial Neurons / perceptron -: 1957  



ML -> Dl -> Ai



Unsupervised ML -: process of Extracting patterns from Unlabelled data
Supervised ML	-: process of making prediction using labelled data



Supervised DEEP learning---- see human brain anatomy---Temporal lobe green color----
Temporal lobe -: data processing -> ANN -: Artificial Neural Network
Occipital lobe -:---blue color---- Visson -> CNN -: Convolutional Neural Network
Frontal Lobe -:------short term long term memory---- Analytics reasoning LSTM -> RNN -> Recurrent Neural network


========Lecturte 2---------------

Gradient Descent----

Introduction to NN and human perceptrons--

And then we will seee how NN works for classification and regression problems.
In dl we try to mimic human brain.. and a neuron is a smallest unit of brain.-----------------------------untitled.png----draw how neuron looks like---
explain dendrides axon, axon terminals-- we have around 86 billion neurons-- there is some gap between axon terminals and and dendrides--This type of connection 
is known as synapse. 

Now, 1 neuron can be connected to another neuron or to muscle tissues of glands or to nerve cells.

Now if we try to draw this geometrically, it looks something like ---

dendrites works as receiver, axon terminal acts as transmitter and the nucleus at the center is where all processing happens.

Draw fwd propogation diag---

b is bias, x1, x2...xn are i/ps or signals -- i/ps have weights., also caleed as sinapse,,.blue circle is neuron,which contains activation func 
which acts on weighted sum of i/ps to give the o/p--
i.e. ypred--- Now this ypred can be categorical or binary or continuous--Then we check how much error we received which we want to reduce-- and based on which we decide its a good or bad model--- 
Now, this type of flow is forward propogation--
Now, what we will to deal with error---- we will travel backward and adjust our weights and bias ---after this step our error lessens--we will go on 
updating our weights and biases until the error minimize or error is least. This is backward propogation.

Now, 1 feed forward propogation has 1 forward propogation and 1 back propogation

Now, implementation of fwd and backward propogation in ANN is called as perceptron algo-- because perceptron doesnt need manual intervention for adjustment of weights.
It can learn and adjust its weight by itself 

But how brain functions---
Computation model is inspired by human brains-- 1. massively parallel distribution system made above the simple processing unit.
2, Synaptic connections strengthen amongst the neuron which is used to store aquired knowledge--
3. Knowledge is aquired by the network from its environment through its learning process through a learning process.

untitlted2.png-------------explain that-- Y=wX+b---

#--------------------------lecture 3----

Same ann can help u to solve both classification and regression problems. This is Fwd Propogation.



Gradient descent is an algorithm for finding the minimum of a differentiable function. 
It takes steps in the steepest downhill direction until minima is reached.
Gradient descent is an iterative process, to find the minimum of a function we take steps proportional to the approximate gradient of the function 
at the current point.


Types of gradient descent


Batch gradient descent  

Start with any random w value.
Calculate gradient G of the function f(w) for that w value, using all data points in the entire training data set.
Update the w value.
This will consume huge training time.
  

Stochastic gradient descent

Start with any random w value.
Calculate gradient G of the function f(w) for that w value, using any one random data point from the entire training data set.
Update the w value.
Much faster than batch gradient descent.


Mini-batch gradient descent

Mini-batch gradient descent is a combination of the stochastic gradient descent and batch gradient descent. It splits the training data into small batches and performs the update on these batches. Therefore it results in a balance between the efficiency of batch gradient descent and the robustness of stochastic gradient descent.


Gradient Descent helps to reduce error--In the 1st fwd pro the error that i am receiving is lets say 5-- now i want to reach a point where errror is 0---
and currently we are at error 5 -- So we iterate such that this error comes down-- 5,4,3,2,1,0.. ie. in each iteration we are learning-- with a learning rate lr,
because of which error is decreasing. now, if this learning rate is too slow, then we will take alot of iterations to reach 0 or we might never reach 0.
And if this lr is too high..then.. the gap between n and n+1 will be high.. and in an iteration it is possible that u will miss the global minima because u r 
taking bigger steps. Our main goal is to reach 0 with min efforts and min error..by default lr is around 5%.

Now, we are at 0, but how we will know that we are at best place having min error. Lets say at n+5 my error is min, then to know this is a min error, we 
should must take one more step-- i.e. n+5 is 0 eror and n+6 is 1 error , so we say at n+5 we are at best place..

This is what gradient descent does. It helps us to reach a point where error is min.

So, what we do is, we have feature x, we train the model,check the score, adjust weight to minimze err and retrain the model. In this model both fwd and backward
propo is done.This is an iterative process to minimize err.

In ML we train our model on complete training dataset. BUt here we do batch wise training..Now, gradient descent uses t2o functions.. 1 is convex func, 2 is non 
convex func---the convex func follows u trajectory and finding global minima is easy... but in case of non convex func.. it has many ups and downs...
------untitled(2).png----explain
and when model is at 1st down.. model thinks this is the min...and it will never reach global minima..by default gradient descent use convex func...
but it depends.. if on a given dataset we are interested on local minima then we go for non convex func...

Now, in backward prop we assign new weights to train our model-- the formula for weight updation is---

new weight=old weight-lr*(derivative of mse wrt weight i.e d(mse)/dw)

--------GradientDescent.ipynb ---

Now, activation func-- y=mx+c, mse between (y and ypred)
in backward prop-- we will adjust weights


----------------Lecture 4----------------

Lets suppose we have dataset with few cols and target.

c1, c2, c3, c4 and t------------untitled.png

Now, w=if we have aroung 2000 rows then we divide it into 70 , 30 %..now again 70% data is divided into different batches with 1 batch containing 30 rows.
1400/30 = 46 batches

x1,x2,x3 are i/ps and 1,2,3,4,5,6 are part of hidden layers


tensorflow_intro.ipynb-----------------------------------------------------------------------------------

tensorflow.keras import Sequential helps in initializing ann, and Dense helps in creating hidden layer.
NOw, each x1, x2 x3 are conected to all the neurons .. now i/p from dense layers are directed for o/p


-----
Modeling Artificial Neural Network

4.1 Artificial Neural Network

An artificial neural network is a combination of multiple layers of neurons, each layer can have multiple neurons. 
These neurons are interconnected with the neurons in the other layers. 

The Input layer has multiple neurons, one for each feature. The Output layer can either have one neuron for regression and binary classification or 
multiple neurons, one for each class in case of multi-class classification. All the other layers in between are the Hidden layers.

The output from each layer of neurons becomes the input of the next layer of neurons.

Now that we have understood what an artificial neural network is, the question here is, why do we need neural networks when a single neuron can be used for 
regression and classification?

Well, a single neuron can only learn linearly separable patterns. 
Most of the time the data is non-linear in nature, this is where the neural networks come into the picture. 


The first layer is a linear combination of the input. 
However, the activation function provides a non-linear output. The next layer combines the non-linear outputs of the first layer in a linear way 
i.e weighted sum of the values. Hence neural networks can fit nonlinear functions.



---ANNR_on_fake_reg_.ipynb----

---ANNC_on_churn_modelling.ipynb--






#---------------Lecture 5

 Most of the time the data is non-linear in nature, this is where the neural networks come into the picture. 


The first layer is a linear combination of the input. However, the activation function provides a non-linear output. 
The next layer combines the non-linear outputs of the first layer in a linear way i.e weighted sum of the values. 
Hence neural networks can fit nonlinear functions.


4.2 Activation functions-----------activation functions.png----- Captures complex relationship between data

Threshold function--- almost like sigmoid func,  but the edge is very sharp here.

f(x)=1if x>=0
    =0 if x<0

Sigmoid function
A sigmoid function is a mathematical function having a characteristic "S"-shaped curve or sigmoid curve, 
the output of a sigmoid function is always between 0 - 1. Hence it can be used as an activation function for
binary classification keeping a threshold value of 0.5, if sigmoid function output is >= 0.5 then it is considered a positive class 1 or else negative class 0.


sigma(x)=1/(1+e^-x)

i/p range : -inf to +inf
o/p range 1 if x>=0, =0 if x<0
derivative : sigma(x)(1-sigma(x))=dsigma/dx
The derivatives or the gradient values for this activation functions are pretty small. It returns probabilitites for the o/p class.

The sigmoid function can be used as an activation function in the output layer, but should not be used in the hidden layers. 
The reason is, the range of the input value passed through the input layer lies between -1 to 1  ( after standardization ), 
if the sigmoid function is used in the hidden layers then the same value will be converted into a range of 0 to 1, 
breaking the uniformity of the neural network. This is where the hyperbolic tangent function comes into the picture.


Hyperbolic tangent function

The hyperbolic tangent function is a mathematical function, this function is easily defined as the ratio between the hyperbolic sine and the cosine functions. 
The output of this function always lies between -1 to 1, making it suitable for the hidden layers to maintain the uniformity of neural networks. 

f(x)=tanh(x)=f(x)=2/(1+e^-2x) - 1
but.. 
sigma(x)=1/(1+e^-x)==> tanh(x)=2sigma(2x)-1
i/p range : -inf to +inf
o/p range : -1 to 1

dtanh/dx=1-tanh^2*(x)

Here, gradient or derivative values are larger than sigmoid activation function value.


But both sigmoid and hyperbolic tangent functions face the vanishing gradient problem. 

The vanishing gradient problem is encountered when training artificial neural networks with gradient-based learning methods and backpropagation. 
In such methods, each of the neural network's weights receives an update proportional to the partial derivative of the error function with respect 
to the current weight in each iteration of training. The problem is that in the case of the sigmoid and hyperbolic tangent function, 
the gradient will be vanishingly small, effectively preventing the weight from changing its value. 
In the worst case, this may completely stop the neural network from further training.

##The Maximum value of the derivative of the sigmoid function is just 0.25.




To avoid vanishing gradient problems, the Rectified Linear Unit function comes into the picture.

Rectified linear unit(ReLu)

f(x)=max(0,x)
i/p range : -inf to +inf
o/p range : 0 to inf
dReLu/dx= 0 for x<0, 1 for x>0,
   	  Not defined for x=0 or 0 for x=0
i.e. the -ve neurons donot get activated during forward propogation and some of the weights and biases donot get updated during back propogation.

The rectified linear activation function is a piecewise linear function that will output the input directly if it is positive, 
otherwise, it will output zero. It has become the default activation function for many types of neural networks because a model that uses it 
is easier to train and often achieves better performance.


Leaky Relu(x)=0.01x for x<0 and x for x>=0 .. tf, LeakyRelu(x)=max(0.01x,x)
dLeakyRelu(x)/dx=0.01 x<=0, 1 x>0

Both ReLu and LeakyRelu are used in hidden layers.


For multicalss classification-- we use softmax activation func : exp(z_i)/summation_j(z_j)
it calculates relative probabilities.
when the no. of calsses is 2, it becomes same as sigmoid activation func.


The softmax function is a function that turns a vector of K real values into a vector of K real values that sum to 1. 
The input values can be positive, negative, zero, or greater than one, but the softmax transforms them into values between 0 and 1 so that 
they can be interpreted as probabilities. If one of the inputs is small or negative, the softmax turns it into a small probability, and if the input is large, 
then it turns it into a large probability, but it will always remain between 0 and 1.



The softmax function can be used as an activation function in the output layer for the multi-class classification problems. 
The output layers must consist of n number of neurons for n different classes. The softmax function will return a probability value 
for each neuron in the output layer. Since each neuron represents a class, the final output will be the class with the highest probability value.  


Now,when to use which function i.e which activation func to  use in hidden layer and which one in o/p layer---

ANN--Relu in hidden layer, and if its a classification problem then u can go for Sigmoid,in case ofregression problemnoneed of any activation funct on o/p layer. 

CNN--in hidden layer,depending on the problem u can choose relu,Sigmoid or softmax at o/p layer

RNN--Sig,Tanh in hidden layer for o/p its a continuous value, so we donot need to give any activation func on o/p layer

Now,in case of breast-cancer dataset, we have target as benign or malignant-- tell me which activation func u r going to use in o/p layer?
sigmoid or threshold--because it has 2 labels

Sigmoid and tangent are avoided sometimes due to vanishing gradient problems.

Relu works in both regression or classification problem.



---
 Loss functions
------

Mean squared error= 1/n(all the values 1 to n * (y-yi)^2)

In statistics, the mean squared error (MSE) or mean squared deviation (MSD) of an estimator measures the average of the squares of the errors 
i.e the average squared difference between the estimated values and the actual value. The MSE is the loss function that measures the quality of an estimator, 
it is always non-negative, and values closer to zero are better. MSE can be used as a loss function for the regression problems.
 

 
Binary cross-entropy =1/n(all the values 1 to n * yi*log(p(yi)) + (1-yi)*log(p(1-yi))  )

Since MSE measures the squared distance between actual and predicted value, it is not suitable for classification, as classification predicts binary or categorical output. Binary cross-entropy is used as a loss function for binary classification.
The cross-entropy value is high whenever there is a misclassification. 




Categorical cross-entropy  

Similarly, categorical cross-entropy is the loss function for multi-class classification. 
= -(1/n)* summation i to n * summation j to m(y_ij log(p_ij)), where n is no. of rows and m is no. of classes




Optimizers------

1.Gradient Descent
The goal here is to minimize the error rate and it uses sumof residuals to minimze the error.

theta_i=theta_i - alpha*dJ/dtheta_i

where, theta_i is parameter, alpha=learning_rate  and dJ/dtheta_i is gradient or slope
i.e 
Mean squared error= 1/n(sum of all the values 1 to n * (y-yi)^2)

Now, if we have 1 million data in our dataset,then to process this data,it might use 1 million micro secs to compute-- hence it is time consuming .
 

Stochastic GD---Uses only 1 point to calc the error.

(y-yp).. but this doesnot give exact picture of how model is behaving..

MiniBatch GD---


Mini Batch Gradient Descent instead of going over all examples, it sums up over lower number of examples based on the batch size and 
performs an update for each of these batches. - 

Mean squared error= 1/k(sum of selected datapoints 1 to k * (y-yi)^2)  where k<n


------------------See the diag in GDandOptimizers.png----------- Explaind in diary as well---



Problems with Gradient Descent are:
1. getting stuck at local minima
2. Same leraning rate throughout the training process.

There have been proposed different options to automate this tuning. One of the successful scheme is AdaGrad.

AdaGrad’s algorithms dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning.

Adaptive Gradient Algorithm (Adagrad) is an algorithm for gradient-based optimization. 
The learning rate is adapted component-wise to the parameters by incorporating knowledge of past observations.  
It performs larger updates (e.g. high learning rates) for those parameters that are related to infrequent features and smaller updates (i.e. low learning rates) 
for frequent one. It performs smaller updates As a result, it is well-suited when dealing with sparse data (NLP or image recognition) 
Each parameter has its own learning rate that improves performance on problems with sparse gradients.

Advantages of Using AdaGrad
It eliminates the need to manually tune the learning rate
Convergence is faster and more reliable – than simple SGD when the scaling of the weights is unequal
It is not very sensitive to the size of the master step


----U can show SGD_GIF file-----

Optimizers-----------------------------------

Gradient Descent with Momentum--

Momentum
A very popular technique that is used along with SGD is called Momentum. 
Instead of using only the gradient of the current step to guide the search, momentum also accumulates the gradient of the past steps to determine the direction to go.
The equations of gradient descent are revised as follows.
i.e. as the training progress, the learning rate is updated as per the cost function . We use sum of the past gradients neta,=
neta=summation of i=1 to t-1[dJ/dtheta]^2
, and use this to update the learning rate.

Squaring is done to remove the effect of +ve and -ve.

and now, instead of plain Gradient Descent, theta_i=theta_i - alpha*dJ/dtheta_i , we use- theta_i=theta_i - (alpha/sqrt(neta))*dJ/dtheta_i

but after sometime, (alpha/sqrt(neta))*dJ/dtheta_i tends to zero, which will lead to convergence, therefore, we can use the weighted sum of gradients,
i.e. some weight is given to the previous gradients and some to the current gradients---

v_t = beta*v_(t-1) + (1-beta)*dJ/dtheta

where, v_(t-1) is previous gradient

where theta_i=theta_i-alpha*v_t

The first equations has two parts. vj is the gradient that is retained from previous iterations. 
This retained gradient is multiplied by a value called "Coefficient of Momentum" which is the percentage of the gradient retained every iteration.

SGD with momentum solves the problem of getting stuck at local minima using the sum of previously accumulated gradients.

Almost always, gradient descent with momentum converges faster than the standard gradient descent algorithm. 
In the standard gradient descent algorithm, you would be taking larger steps in one direction and smaller steps in another direction which slows down the algorithm. 
In the image shown below, you can see that standard gradient descent takes larger steps in the y- direction and smaller steps in the x-direction. 
If our algorithm is able to reduce the steps taken in the y-direction and concentrate the direction of the step in the x-direction, 
our algorithm would converge faster. This is what momentum does, it restricts the oscillation in one direction so that our algorithm can converge faster. 
Also, since the number of steps taken in the y-direction is restricted, we can set a higher learning rate.

show the convergence of gradient descent---slow but very straight line

convergence of mini batch gd is zigzag line because of too much convergence of each batch and might take more time to converge.



2.RMSProp

In order to reduce convergence and adjust learning rate,i.e. as my error is reducing iwant to reduce my learning rate too. This algo is calledas RMSProp.

The RMSprop optimizer is similar to the gradient descent algorithm with momentum. The RMSprop optimizer restricts the oscillations in the vertical direction. 
Therefore, we can increase our learning rate and our algorithm could take larger steps in the horizontal direction converging faster. 
The difference between RMSprop and gradient descent is on how the gradients are calculated. 
The following equations show how the gradients are calculated for the RMSprop and gradient descent with momentum. 
The value of momentum is denoted by beta and is usually set to 0.9. If you are not interested in the math behind the optimizer, 

mu_t=beta*mu_(t-1)+(1-beta)*[dJ/dtheta]^2==================>squaring is done to remove the effect of sign
theta_i=theta_i-alpha/(sqrt(mu_t)+epsilon)*(dJ/dtheta) where, theta_i is the sum of sqrs of previous gradients.

now, when, sq of gradients is increasing, mu_t is increasing, therefore, lr is decreasing, i.e. alpha/(sqrt(mu_t)+epsilon) decrease. 

Sometimes the value of mu_(t-1) could be really close to 0. Then, the value of our weights could blow up. 
To prevent the gradients from blowing up, we include a parameter epsilon in the denominator which is set to a small value.

i.e. in rmsprop,if error is more ,then lr is high,if error is lesser then lr is small.



Now, zig zag convergence of minibatch gd is more smooth because the high convergence rate of mini batch is reduced. This convergence rate is controlled by mometum.

RMSProp is the default optimizer,which we can change to Adams Optimizer. 

Adam optimizer

The Adam optimization algorithm is an extension to stochastic gradient descent that has recently seen broader adoption for deep learning applications 
in computer vision and natural language processing.

Stochastic gradient descent maintains a single learning rate for all weight updates and the learning rate does not change during training. 
On the other hand, Adam is an adaptive learning rate optimization algorithm. It is much faster than the stochastic gradient descent. 
Hence making is suitable for large neural networks.

Now, rmsprop reduce lr, abd the by default optimizer in NN is rmsprop.

But we can change it to adam--- it is the combination of rmsprop and SGD with momentum -

SGD_with_momentum=v_j = beta1*v_(t-1) + (1-beta1)*dJ/dtheta

and, RMSProp=mu_t=beta2*mu_(t-1)+(1-beta2)*[dJ/dtheta]^2,

here, beta1 and beta2 are different,

theta_i = theta_(i-1) - (alpha/(sqrt(mu_t+epsilon))) * v_t

where, v_t is the weighted sum of current and previous gradients and
mu_t contains sum of squares of previous gradients  which is used to scale the learning rate.

Here, we compute the exponential average of the gradient as well as the squares of the gradient for each parameters (Eq 1, and Eq 2). 
To decide our learning step, we multiply our learning rate by average of the gradient (as was the case with momentum) 
and divide it by the root mean square of the exponential average of square of gradients (as was the case with momentum) in equation 3. Then, we add the update.

The hyperparameter beta1 is generally kept around 0.9 while beta_2 is kept at 0.99. Epsilon is chosen to be 1e-10 generally.

This is most used ..if u only want to adjust lr ,go with rmsprop,

and if u want to adjust both lr and momentum use Adm.


https://towardsdatascience.com/complete-guide-to-adam-optimization-1e5f29532c3d


----------Show GD_Modmentum_Adgrad_RMSProp_Adam_Working.gif

----------------------------------------------------Part2.webm----

ann_chp-on_cancer_classification.ipynb

------------------------------------6th folder----


tensorflow_intro.ipynb

neural_network from scratch using numpy for fwd and backward propogation

95SOLV~1_IPY.ipynb ------on loan_prediction_data

DeepLearning_on_loan_prediction_with_hyperparameter_tuning_model_improvement.ipynb


---ANNC_on_churn_modelling.ipynb--

---todaymmc_softmax_on_iris.ipynb

---ANNR_on_fake_reg_.ipynb----
============================================================
==============================================================
=================================================================
====================================================================


input_neurons=xtrain.shape[1]
op_neurons=1
number_of_hidden_layers=2
neuron_hidden_layer_1=10
neuron_hidden_layer_2=5

model=Sequential()
model.add(InputLayer(input_shape=(input_neurons,)))
model.add(Dense(units=neuron_hidden_layer_1,activation='relu'))
model.add(Dense(units=neuron_hidden_layer_2,activation='relu'))
model.add(Dense(units=op_neurons,activation='sigmoid'))

model.summary()

model.compile(loss='binary_crossentropy',optimizer="Adam",metrics=['accuracy'])

model_history=model.fit(xtrain,ytrain,validation_data=(xtest,ytest),epochs=50)

prediction=model.predict_classes(xtest)

accuracy_score(ytest,prediction)

plt.plot(model_history.history['loss'])
plt.plot(model_history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train','validation'],loc='upper left')
plt.show()

plt.plot(model_history.history['acc'])
plt.plot(model_history.history['val_acc'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train','validation'],loc='upper left')
plt.show()






 


