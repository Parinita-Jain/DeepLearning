{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "yN7z1OoHRX2i"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import warnings \n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Sequential"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "63dIlxdxSULZ",
        "outputId": "8116ee67-c8a2-479a-ef4f-be654bab677e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
              "0        17.99         10.38          122.80     1001.0          0.11840   \n",
              "1        20.57         17.77          132.90     1326.0          0.08474   \n",
              "2        19.69         21.25          130.00     1203.0          0.10960   \n",
              "3        11.42         20.38           77.58      386.1          0.14250   \n",
              "4        20.29         14.34          135.10     1297.0          0.10030   \n",
              "\n",
              "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
              "0           0.27760          0.3001              0.14710         0.2419   \n",
              "1           0.07864          0.0869              0.07017         0.1812   \n",
              "2           0.15990          0.1974              0.12790         0.2069   \n",
              "3           0.28390          0.2414              0.10520         0.2597   \n",
              "4           0.13280          0.1980              0.10430         0.1809   \n",
              "\n",
              "   mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
              "0                 0.07871  ...          17.33           184.60      2019.0   \n",
              "1                 0.05667  ...          23.41           158.80      1956.0   \n",
              "2                 0.05999  ...          25.53           152.50      1709.0   \n",
              "3                 0.09744  ...          26.50            98.87       567.7   \n",
              "4                 0.05883  ...          16.67           152.20      1575.0   \n",
              "\n",
              "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
              "0            0.1622             0.6656           0.7119                0.2654   \n",
              "1            0.1238             0.1866           0.2416                0.1860   \n",
              "2            0.1444             0.4245           0.4504                0.2430   \n",
              "3            0.2098             0.8663           0.6869                0.2575   \n",
              "4            0.1374             0.2050           0.4000                0.1625   \n",
              "\n",
              "   worst symmetry  worst fractal dimension  target  \n",
              "0          0.4601                  0.11890       0  \n",
              "1          0.2750                  0.08902       0  \n",
              "2          0.3613                  0.08758       0  \n",
              "3          0.6638                  0.17300       0  \n",
              "4          0.2364                  0.07678       0  \n",
              "\n",
              "[5 rows x 31 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-972378de-d5e2-49d6-8cb9-4956f827c5b1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mean radius</th>\n",
              "      <th>mean texture</th>\n",
              "      <th>mean perimeter</th>\n",
              "      <th>mean area</th>\n",
              "      <th>mean smoothness</th>\n",
              "      <th>mean compactness</th>\n",
              "      <th>mean concavity</th>\n",
              "      <th>mean concave points</th>\n",
              "      <th>mean symmetry</th>\n",
              "      <th>mean fractal dimension</th>\n",
              "      <th>...</th>\n",
              "      <th>worst texture</th>\n",
              "      <th>worst perimeter</th>\n",
              "      <th>worst area</th>\n",
              "      <th>worst smoothness</th>\n",
              "      <th>worst compactness</th>\n",
              "      <th>worst concavity</th>\n",
              "      <th>worst concave points</th>\n",
              "      <th>worst symmetry</th>\n",
              "      <th>worst fractal dimension</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.3001</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>0.2419</td>\n",
              "      <td>0.07871</td>\n",
              "      <td>...</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.1622</td>\n",
              "      <td>0.6656</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.0869</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>0.1812</td>\n",
              "      <td>0.05667</td>\n",
              "      <td>...</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.1238</td>\n",
              "      <td>0.1866</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.1974</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>0.05999</td>\n",
              "      <td>...</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.1444</td>\n",
              "      <td>0.4245</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.2414</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>0.2597</td>\n",
              "      <td>0.09744</td>\n",
              "      <td>...</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.2098</td>\n",
              "      <td>0.8663</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.1980</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>0.05883</td>\n",
              "      <td>...</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.1374</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 31 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-972378de-d5e2-49d6-8cb9-4956f827c5b1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-972378de-d5e2-49d6-8cb9-4956f827c5b1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-972378de-d5e2-49d6-8cb9-4956f827c5b1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "df = pd.read_csv(\"cancer_classification.csv\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXB0gEnbSm7-",
        "outputId": "c121c189-0f6c-4358-f2e8-dc2334bd0334"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 569 entries, 0 to 568\n",
            "Data columns (total 31 columns):\n",
            " #   Column                   Non-Null Count  Dtype  \n",
            "---  ------                   --------------  -----  \n",
            " 0   mean radius              569 non-null    float64\n",
            " 1   mean texture             569 non-null    float64\n",
            " 2   mean perimeter           569 non-null    float64\n",
            " 3   mean area                569 non-null    float64\n",
            " 4   mean smoothness          569 non-null    float64\n",
            " 5   mean compactness         569 non-null    float64\n",
            " 6   mean concavity           569 non-null    float64\n",
            " 7   mean concave points      569 non-null    float64\n",
            " 8   mean symmetry            569 non-null    float64\n",
            " 9   mean fractal dimension   569 non-null    float64\n",
            " 10  radius error             569 non-null    float64\n",
            " 11  texture error            569 non-null    float64\n",
            " 12  perimeter error          569 non-null    float64\n",
            " 13  area error               569 non-null    float64\n",
            " 14  smoothness error         569 non-null    float64\n",
            " 15  compactness error        569 non-null    float64\n",
            " 16  concavity error          569 non-null    float64\n",
            " 17  concave points error     569 non-null    float64\n",
            " 18  symmetry error           569 non-null    float64\n",
            " 19  fractal dimension error  569 non-null    float64\n",
            " 20  worst radius             569 non-null    float64\n",
            " 21  worst texture            569 non-null    float64\n",
            " 22  worst perimeter          569 non-null    float64\n",
            " 23  worst area               569 non-null    float64\n",
            " 24  worst smoothness         569 non-null    float64\n",
            " 25  worst compactness        569 non-null    float64\n",
            " 26  worst concavity          569 non-null    float64\n",
            " 27  worst concave points     569 non-null    float64\n",
            " 28  worst symmetry           569 non-null    float64\n",
            " 29  worst fractal dimension  569 non-null    float64\n",
            " 30  target                   569 non-null    int64  \n",
            "dtypes: float64(30), int64(1)\n",
            "memory usage: 137.9 KB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "GYUvNnTTSsbV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "291fb884-ab1c-4570-876d-c317f50f8106"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "mean radius                0\n",
              "mean texture               0\n",
              "mean perimeter             0\n",
              "mean area                  0\n",
              "mean smoothness            0\n",
              "mean compactness           0\n",
              "mean concavity             0\n",
              "mean concave points        0\n",
              "mean symmetry              0\n",
              "mean fractal dimension     0\n",
              "radius error               0\n",
              "texture error              0\n",
              "perimeter error            0\n",
              "area error                 0\n",
              "smoothness error           0\n",
              "compactness error          0\n",
              "concavity error            0\n",
              "concave points error       0\n",
              "symmetry error             0\n",
              "fractal dimension error    0\n",
              "worst radius               0\n",
              "worst texture              0\n",
              "worst perimeter            0\n",
              "worst area                 0\n",
              "worst smoothness           0\n",
              "worst compactness          0\n",
              "worst concavity            0\n",
              "worst concave points       0\n",
              "worst symmetry             0\n",
              "worst fractal dimension    0\n",
              "target                     0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "df.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSHfqbduS4PX",
        "outputId": "56096407-1abc-494b-cbe3-15b1069f60b1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    357\n",
              "0    212\n",
              "Name: target, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "df.target.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_-9DvIHzS-3Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "66a7981b-2682-4e55-f053-b83bac64a298"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f4a3fa5b340>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "sns.countplot(x=\"target\", data=df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "2HW_fxm_THIA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "3f6f656b-b742-420c-d54f-39edbfed0039"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       mean radius  mean texture  mean perimeter    mean area  \\\n",
              "count   569.000000    569.000000      569.000000   569.000000   \n",
              "mean     14.127292     19.289649       91.969033   654.889104   \n",
              "std       3.524049      4.301036       24.298981   351.914129   \n",
              "min       6.981000      9.710000       43.790000   143.500000   \n",
              "25%      11.700000     16.170000       75.170000   420.300000   \n",
              "50%      13.370000     18.840000       86.240000   551.100000   \n",
              "75%      15.780000     21.800000      104.100000   782.700000   \n",
              "max      28.110000     39.280000      188.500000  2501.000000   \n",
              "\n",
              "       mean smoothness  mean compactness  mean concavity  mean concave points  \\\n",
              "count       569.000000        569.000000      569.000000           569.000000   \n",
              "mean          0.096360          0.104341        0.088799             0.048919   \n",
              "std           0.014064          0.052813        0.079720             0.038803   \n",
              "min           0.052630          0.019380        0.000000             0.000000   \n",
              "25%           0.086370          0.064920        0.029560             0.020310   \n",
              "50%           0.095870          0.092630        0.061540             0.033500   \n",
              "75%           0.105300          0.130400        0.130700             0.074000   \n",
              "max           0.163400          0.345400        0.426800             0.201200   \n",
              "\n",
              "       mean symmetry  mean fractal dimension  ...  worst texture  \\\n",
              "count     569.000000              569.000000  ...     569.000000   \n",
              "mean        0.181162                0.062798  ...      25.677223   \n",
              "std         0.027414                0.007060  ...       6.146258   \n",
              "min         0.106000                0.049960  ...      12.020000   \n",
              "25%         0.161900                0.057700  ...      21.080000   \n",
              "50%         0.179200                0.061540  ...      25.410000   \n",
              "75%         0.195700                0.066120  ...      29.720000   \n",
              "max         0.304000                0.097440  ...      49.540000   \n",
              "\n",
              "       worst perimeter   worst area  worst smoothness  worst compactness  \\\n",
              "count       569.000000   569.000000        569.000000         569.000000   \n",
              "mean        107.261213   880.583128          0.132369           0.254265   \n",
              "std          33.602542   569.356993          0.022832           0.157336   \n",
              "min          50.410000   185.200000          0.071170           0.027290   \n",
              "25%          84.110000   515.300000          0.116600           0.147200   \n",
              "50%          97.660000   686.500000          0.131300           0.211900   \n",
              "75%         125.400000  1084.000000          0.146000           0.339100   \n",
              "max         251.200000  4254.000000          0.222600           1.058000   \n",
              "\n",
              "       worst concavity  worst concave points  worst symmetry  \\\n",
              "count       569.000000            569.000000      569.000000   \n",
              "mean          0.272188              0.114606        0.290076   \n",
              "std           0.208624              0.065732        0.061867   \n",
              "min           0.000000              0.000000        0.156500   \n",
              "25%           0.114500              0.064930        0.250400   \n",
              "50%           0.226700              0.099930        0.282200   \n",
              "75%           0.382900              0.161400        0.317900   \n",
              "max           1.252000              0.291000        0.663800   \n",
              "\n",
              "       worst fractal dimension      target  \n",
              "count               569.000000  569.000000  \n",
              "mean                  0.083946    0.627417  \n",
              "std                   0.018061    0.483918  \n",
              "min                   0.055040    0.000000  \n",
              "25%                   0.071460    0.000000  \n",
              "50%                   0.080040    1.000000  \n",
              "75%                   0.092080    1.000000  \n",
              "max                   0.207500    1.000000  \n",
              "\n",
              "[8 rows x 31 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cad4515c-4fa3-4c29-890a-429eb664ada5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mean radius</th>\n",
              "      <th>mean texture</th>\n",
              "      <th>mean perimeter</th>\n",
              "      <th>mean area</th>\n",
              "      <th>mean smoothness</th>\n",
              "      <th>mean compactness</th>\n",
              "      <th>mean concavity</th>\n",
              "      <th>mean concave points</th>\n",
              "      <th>mean symmetry</th>\n",
              "      <th>mean fractal dimension</th>\n",
              "      <th>...</th>\n",
              "      <th>worst texture</th>\n",
              "      <th>worst perimeter</th>\n",
              "      <th>worst area</th>\n",
              "      <th>worst smoothness</th>\n",
              "      <th>worst compactness</th>\n",
              "      <th>worst concavity</th>\n",
              "      <th>worst concave points</th>\n",
              "      <th>worst symmetry</th>\n",
              "      <th>worst fractal dimension</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>569.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>14.127292</td>\n",
              "      <td>19.289649</td>\n",
              "      <td>91.969033</td>\n",
              "      <td>654.889104</td>\n",
              "      <td>0.096360</td>\n",
              "      <td>0.104341</td>\n",
              "      <td>0.088799</td>\n",
              "      <td>0.048919</td>\n",
              "      <td>0.181162</td>\n",
              "      <td>0.062798</td>\n",
              "      <td>...</td>\n",
              "      <td>25.677223</td>\n",
              "      <td>107.261213</td>\n",
              "      <td>880.583128</td>\n",
              "      <td>0.132369</td>\n",
              "      <td>0.254265</td>\n",
              "      <td>0.272188</td>\n",
              "      <td>0.114606</td>\n",
              "      <td>0.290076</td>\n",
              "      <td>0.083946</td>\n",
              "      <td>0.627417</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>3.524049</td>\n",
              "      <td>4.301036</td>\n",
              "      <td>24.298981</td>\n",
              "      <td>351.914129</td>\n",
              "      <td>0.014064</td>\n",
              "      <td>0.052813</td>\n",
              "      <td>0.079720</td>\n",
              "      <td>0.038803</td>\n",
              "      <td>0.027414</td>\n",
              "      <td>0.007060</td>\n",
              "      <td>...</td>\n",
              "      <td>6.146258</td>\n",
              "      <td>33.602542</td>\n",
              "      <td>569.356993</td>\n",
              "      <td>0.022832</td>\n",
              "      <td>0.157336</td>\n",
              "      <td>0.208624</td>\n",
              "      <td>0.065732</td>\n",
              "      <td>0.061867</td>\n",
              "      <td>0.018061</td>\n",
              "      <td>0.483918</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>6.981000</td>\n",
              "      <td>9.710000</td>\n",
              "      <td>43.790000</td>\n",
              "      <td>143.500000</td>\n",
              "      <td>0.052630</td>\n",
              "      <td>0.019380</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.106000</td>\n",
              "      <td>0.049960</td>\n",
              "      <td>...</td>\n",
              "      <td>12.020000</td>\n",
              "      <td>50.410000</td>\n",
              "      <td>185.200000</td>\n",
              "      <td>0.071170</td>\n",
              "      <td>0.027290</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.156500</td>\n",
              "      <td>0.055040</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>11.700000</td>\n",
              "      <td>16.170000</td>\n",
              "      <td>75.170000</td>\n",
              "      <td>420.300000</td>\n",
              "      <td>0.086370</td>\n",
              "      <td>0.064920</td>\n",
              "      <td>0.029560</td>\n",
              "      <td>0.020310</td>\n",
              "      <td>0.161900</td>\n",
              "      <td>0.057700</td>\n",
              "      <td>...</td>\n",
              "      <td>21.080000</td>\n",
              "      <td>84.110000</td>\n",
              "      <td>515.300000</td>\n",
              "      <td>0.116600</td>\n",
              "      <td>0.147200</td>\n",
              "      <td>0.114500</td>\n",
              "      <td>0.064930</td>\n",
              "      <td>0.250400</td>\n",
              "      <td>0.071460</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>13.370000</td>\n",
              "      <td>18.840000</td>\n",
              "      <td>86.240000</td>\n",
              "      <td>551.100000</td>\n",
              "      <td>0.095870</td>\n",
              "      <td>0.092630</td>\n",
              "      <td>0.061540</td>\n",
              "      <td>0.033500</td>\n",
              "      <td>0.179200</td>\n",
              "      <td>0.061540</td>\n",
              "      <td>...</td>\n",
              "      <td>25.410000</td>\n",
              "      <td>97.660000</td>\n",
              "      <td>686.500000</td>\n",
              "      <td>0.131300</td>\n",
              "      <td>0.211900</td>\n",
              "      <td>0.226700</td>\n",
              "      <td>0.099930</td>\n",
              "      <td>0.282200</td>\n",
              "      <td>0.080040</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>15.780000</td>\n",
              "      <td>21.800000</td>\n",
              "      <td>104.100000</td>\n",
              "      <td>782.700000</td>\n",
              "      <td>0.105300</td>\n",
              "      <td>0.130400</td>\n",
              "      <td>0.130700</td>\n",
              "      <td>0.074000</td>\n",
              "      <td>0.195700</td>\n",
              "      <td>0.066120</td>\n",
              "      <td>...</td>\n",
              "      <td>29.720000</td>\n",
              "      <td>125.400000</td>\n",
              "      <td>1084.000000</td>\n",
              "      <td>0.146000</td>\n",
              "      <td>0.339100</td>\n",
              "      <td>0.382900</td>\n",
              "      <td>0.161400</td>\n",
              "      <td>0.317900</td>\n",
              "      <td>0.092080</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>28.110000</td>\n",
              "      <td>39.280000</td>\n",
              "      <td>188.500000</td>\n",
              "      <td>2501.000000</td>\n",
              "      <td>0.163400</td>\n",
              "      <td>0.345400</td>\n",
              "      <td>0.426800</td>\n",
              "      <td>0.201200</td>\n",
              "      <td>0.304000</td>\n",
              "      <td>0.097440</td>\n",
              "      <td>...</td>\n",
              "      <td>49.540000</td>\n",
              "      <td>251.200000</td>\n",
              "      <td>4254.000000</td>\n",
              "      <td>0.222600</td>\n",
              "      <td>1.058000</td>\n",
              "      <td>1.252000</td>\n",
              "      <td>0.291000</td>\n",
              "      <td>0.663800</td>\n",
              "      <td>0.207500</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 31 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cad4515c-4fa3-4c29-890a-429eb664ada5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-cad4515c-4fa3-4c29-890a-429eb664ada5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-cad4515c-4fa3-4c29-890a-429eb664ada5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "C-f_61-aTQMT"
      },
      "outputs": [],
      "source": [
        "x = df.iloc[:, :-1].values\n",
        "y = df.iloc[:, -1].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "GdLXTmPJTkjY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48dc6591-226a-4b88-e4d4-43d23e222ca9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.799e+01, 1.038e+01, 1.228e+02, ..., 2.654e-01, 4.601e-01,\n",
              "        1.189e-01],\n",
              "       [2.057e+01, 1.777e+01, 1.329e+02, ..., 1.860e-01, 2.750e-01,\n",
              "        8.902e-02],\n",
              "       [1.969e+01, 2.125e+01, 1.300e+02, ..., 2.430e-01, 3.613e-01,\n",
              "        8.758e-02],\n",
              "       ...,\n",
              "       [1.660e+01, 2.808e+01, 1.083e+02, ..., 1.418e-01, 2.218e-01,\n",
              "        7.820e-02],\n",
              "       [2.060e+01, 2.933e+01, 1.401e+02, ..., 2.650e-01, 4.087e-01,\n",
              "        1.240e-01],\n",
              "       [7.760e+00, 2.454e+01, 4.792e+01, ..., 0.000e+00, 2.871e-01,\n",
              "        7.039e-02]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "uWhD8A-8TmC2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb10cedd-dfa6-4336-f2f1-b7c859305a19"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
              "       1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
              "       1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
              "       0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
              "       1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
              "       1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
              "       1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
              "       0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
              "       0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
              "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
              "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
              "       1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
              "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
              "       1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
              "       1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "6wQ8lgqkTnEq"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "xtrain, xtest, ytrain, ytest = train_test_split(x,y, test_size=0.2, random_state=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "UTcO65LeT5bn"
      },
      "outputs": [],
      "source": [
        "# Now before moving with NN,we will scale our data--\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "xtrain = sc.fit_transform(xtrain)\n",
        "xtest = sc.transform(xtest) # fit does the caklc for us,and transformhelpstoapply those transformations\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "hINI43zBUV0Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4258ff58-b454-4fb0-f8cb-6992199bef82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/600\n",
            "15/15 [==============================] - 4s 13ms/step - loss: 0.6226 - val_loss: 0.5128\n",
            "Epoch 2/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4243 - val_loss: 0.3805\n",
            "Epoch 3/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.3032 - val_loss: 0.2900\n",
            "Epoch 4/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.2234 - val_loss: 0.2288\n",
            "Epoch 5/600\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.1736 - val_loss: 0.1901\n",
            "Epoch 6/600\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.1414 - val_loss: 0.1664\n",
            "Epoch 7/600\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.1201 - val_loss: 0.1477\n",
            "Epoch 8/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.1045 - val_loss: 0.1342\n",
            "Epoch 9/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0936 - val_loss: 0.1259\n",
            "Epoch 10/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0850 - val_loss: 0.1171\n",
            "Epoch 11/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0788 - val_loss: 0.1119\n",
            "Epoch 12/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0734 - val_loss: 0.1100\n",
            "Epoch 13/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0690 - val_loss: 0.1067\n",
            "Epoch 14/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0649 - val_loss: 0.1045\n",
            "Epoch 15/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0613 - val_loss: 0.1024\n",
            "Epoch 16/600\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 0.0584 - val_loss: 0.1012\n",
            "Epoch 17/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0557 - val_loss: 0.1015\n",
            "Epoch 18/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0530 - val_loss: 0.0996\n",
            "Epoch 19/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0510 - val_loss: 0.0982\n",
            "Epoch 20/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0488 - val_loss: 0.1018\n",
            "Epoch 21/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0468 - val_loss: 0.0994\n",
            "Epoch 22/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0447 - val_loss: 0.0990\n",
            "Epoch 23/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0429 - val_loss: 0.0984\n",
            "Epoch 24/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0415 - val_loss: 0.0961\n",
            "Epoch 25/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0400 - val_loss: 0.0989\n",
            "Epoch 26/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0377 - val_loss: 0.0934\n",
            "Epoch 27/600\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 0.0365 - val_loss: 0.0935\n",
            "Epoch 28/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0353 - val_loss: 0.0917\n",
            "Epoch 29/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0340 - val_loss: 0.0935\n",
            "Epoch 30/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0323 - val_loss: 0.0949\n",
            "Epoch 31/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0314 - val_loss: 0.0936\n",
            "Epoch 32/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0299 - val_loss: 0.0953\n",
            "Epoch 33/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0291 - val_loss: 0.0950\n",
            "Epoch 34/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0279 - val_loss: 0.0940\n",
            "Epoch 35/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0272 - val_loss: 0.0960\n",
            "Epoch 36/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0261 - val_loss: 0.0942\n",
            "Epoch 37/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0252 - val_loss: 0.0943\n",
            "Epoch 38/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0242 - val_loss: 0.0936\n",
            "Epoch 39/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0235 - val_loss: 0.0939\n",
            "Epoch 40/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0229 - val_loss: 0.0952\n",
            "Epoch 41/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0217 - val_loss: 0.0970\n",
            "Epoch 42/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0211 - val_loss: 0.0977\n",
            "Epoch 43/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0203 - val_loss: 0.0978\n",
            "Epoch 44/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0196 - val_loss: 0.0984\n",
            "Epoch 45/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0190 - val_loss: 0.0967\n",
            "Epoch 46/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0184 - val_loss: 0.0967\n",
            "Epoch 47/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0177 - val_loss: 0.0979\n",
            "Epoch 48/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0173 - val_loss: 0.0970\n",
            "Epoch 49/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0167 - val_loss: 0.0999\n",
            "Epoch 50/600\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 0.0166 - val_loss: 0.1003\n",
            "Epoch 51/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0156 - val_loss: 0.0984\n",
            "Epoch 52/600\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 0.0151 - val_loss: 0.0995\n",
            "Epoch 53/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0146 - val_loss: 0.1000\n",
            "Epoch 54/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0140 - val_loss: 0.1023\n",
            "Epoch 55/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0137 - val_loss: 0.1019\n",
            "Epoch 56/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0134 - val_loss: 0.1026\n",
            "Epoch 57/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0127 - val_loss: 0.0979\n",
            "Epoch 58/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0133 - val_loss: 0.0995\n",
            "Epoch 59/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0120 - val_loss: 0.1029\n",
            "Epoch 60/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0116 - val_loss: 0.1037\n",
            "Epoch 61/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0112 - val_loss: 0.1025\n",
            "Epoch 62/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0108 - val_loss: 0.1022\n",
            "Epoch 63/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0106 - val_loss: 0.1052\n",
            "Epoch 64/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0103 - val_loss: 0.1061\n",
            "Epoch 65/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0099 - val_loss: 0.1091\n",
            "Epoch 66/600\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 0.0098 - val_loss: 0.1053\n",
            "Epoch 67/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0094 - val_loss: 0.1069\n",
            "Epoch 68/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0090 - val_loss: 0.1068\n",
            "Epoch 69/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0089 - val_loss: 0.1065\n",
            "Epoch 70/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0084 - val_loss: 0.1072\n",
            "Epoch 71/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0085 - val_loss: 0.1080\n",
            "Epoch 72/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0081 - val_loss: 0.1085\n",
            "Epoch 73/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0077 - val_loss: 0.1104\n",
            "Epoch 74/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0073 - val_loss: 0.1090\n",
            "Epoch 75/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0071 - val_loss: 0.1101\n",
            "Epoch 76/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0069 - val_loss: 0.1095\n",
            "Epoch 77/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0067 - val_loss: 0.1144\n",
            "Epoch 78/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0065 - val_loss: 0.1134\n",
            "Epoch 79/600\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 0.0063 - val_loss: 0.1118\n",
            "Epoch 80/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0062 - val_loss: 0.1107\n",
            "Epoch 81/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0059 - val_loss: 0.1113\n",
            "Epoch 82/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0057 - val_loss: 0.1127\n",
            "Epoch 83/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0056 - val_loss: 0.1132\n",
            "Epoch 84/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0056 - val_loss: 0.1125\n",
            "Epoch 85/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0053 - val_loss: 0.1152\n",
            "Epoch 86/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0051 - val_loss: 0.1146\n",
            "Epoch 87/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0050 - val_loss: 0.1155\n",
            "Epoch 88/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0048 - val_loss: 0.1131\n",
            "Epoch 89/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0046 - val_loss: 0.1156\n",
            "Epoch 90/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0045 - val_loss: 0.1188\n",
            "Epoch 91/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0044 - val_loss: 0.1173\n",
            "Epoch 92/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0042 - val_loss: 0.1177\n",
            "Epoch 93/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0041 - val_loss: 0.1172\n",
            "Epoch 94/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0040 - val_loss: 0.1214\n",
            "Epoch 95/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0039 - val_loss: 0.1200\n",
            "Epoch 96/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0038 - val_loss: 0.1183\n",
            "Epoch 97/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0037 - val_loss: 0.1210\n",
            "Epoch 98/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0036 - val_loss: 0.1221\n",
            "Epoch 99/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0041 - val_loss: 0.1230\n",
            "Epoch 100/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0034 - val_loss: 0.1211\n",
            "Epoch 101/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.1224\n",
            "Epoch 102/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0032 - val_loss: 0.1226\n",
            "Epoch 103/600\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 0.0031 - val_loss: 0.1240\n",
            "Epoch 104/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0031 - val_loss: 0.1244\n",
            "Epoch 105/600\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 0.0030 - val_loss: 0.1253\n",
            "Epoch 106/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0029 - val_loss: 0.1256\n",
            "Epoch 107/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0028 - val_loss: 0.1276\n",
            "Epoch 108/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0027 - val_loss: 0.1284\n",
            "Epoch 109/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0027 - val_loss: 0.1185\n",
            "Epoch 110/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0026 - val_loss: 0.1210\n",
            "Epoch 111/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0025 - val_loss: 0.1245\n",
            "Epoch 112/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0024 - val_loss: 0.1282\n",
            "Epoch 113/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0024 - val_loss: 0.1277\n",
            "Epoch 114/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0023 - val_loss: 0.1286\n",
            "Epoch 115/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0022 - val_loss: 0.1290\n",
            "Epoch 116/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0023 - val_loss: 0.1276\n",
            "Epoch 117/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0022 - val_loss: 0.1300\n",
            "Epoch 118/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0022 - val_loss: 0.1347\n",
            "Epoch 119/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0021 - val_loss: 0.1314\n",
            "Epoch 120/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0020 - val_loss: 0.1320\n",
            "Epoch 121/600\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.0019 - val_loss: 0.1343\n",
            "Epoch 122/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0018 - val_loss: 0.1352\n",
            "Epoch 123/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0019 - val_loss: 0.1344\n",
            "Epoch 124/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0017 - val_loss: 0.1343\n",
            "Epoch 125/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0017 - val_loss: 0.1350\n",
            "Epoch 126/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0017 - val_loss: 0.1362\n",
            "Epoch 127/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0016 - val_loss: 0.1344\n",
            "Epoch 128/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0016 - val_loss: 0.1369\n",
            "Epoch 129/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0015 - val_loss: 0.1369\n",
            "Epoch 130/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0015 - val_loss: 0.1372\n",
            "Epoch 131/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0015 - val_loss: 0.1309\n",
            "Epoch 132/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0015 - val_loss: 0.1326\n",
            "Epoch 133/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0014 - val_loss: 0.1350\n",
            "Epoch 134/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0014 - val_loss: 0.1373\n",
            "Epoch 135/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0014 - val_loss: 0.1381\n",
            "Epoch 136/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0013 - val_loss: 0.1390\n",
            "Epoch 137/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0013 - val_loss: 0.1398\n",
            "Epoch 138/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0013 - val_loss: 0.1404\n",
            "Epoch 139/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0012 - val_loss: 0.1414\n",
            "Epoch 140/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0012 - val_loss: 0.1356\n",
            "Epoch 141/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0012 - val_loss: 0.1392\n",
            "Epoch 142/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0012 - val_loss: 0.1430\n",
            "Epoch 143/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0011 - val_loss: 0.1431\n",
            "Epoch 144/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0011 - val_loss: 0.1423\n",
            "Epoch 145/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0011 - val_loss: 0.1437\n",
            "Epoch 146/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0010 - val_loss: 0.1449\n",
            "Epoch 147/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0010 - val_loss: 0.1448\n",
            "Epoch 148/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0010 - val_loss: 0.1452\n",
            "Epoch 149/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 9.9100e-04 - val_loss: 0.1463\n",
            "Epoch 150/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 9.7907e-04 - val_loss: 0.1464\n",
            "Epoch 151/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 9.5396e-04 - val_loss: 0.1404\n",
            "Epoch 152/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 9.4466e-04 - val_loss: 0.1419\n",
            "Epoch 153/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 9.1862e-04 - val_loss: 0.1439\n",
            "Epoch 154/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 8.9407e-04 - val_loss: 0.1456\n",
            "Epoch 155/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 8.6608e-04 - val_loss: 0.1469\n",
            "Epoch 156/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 8.5551e-04 - val_loss: 0.1447\n",
            "Epoch 157/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 8.3826e-04 - val_loss: 0.1451\n",
            "Epoch 158/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 8.1854e-04 - val_loss: 0.1479\n",
            "Epoch 159/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 8.0157e-04 - val_loss: 0.1486\n",
            "Epoch 160/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 7.8243e-04 - val_loss: 0.1502\n",
            "Epoch 161/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 7.6975e-04 - val_loss: 0.1515\n",
            "Epoch 162/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 7.6561e-04 - val_loss: 0.1523\n",
            "Epoch 163/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 7.6373e-04 - val_loss: 0.1545\n",
            "Epoch 164/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 7.2988e-04 - val_loss: 0.1546\n",
            "Epoch 165/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 7.0636e-04 - val_loss: 0.1548\n",
            "Epoch 166/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 6.7759e-04 - val_loss: 0.1548\n",
            "Epoch 167/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 6.6927e-04 - val_loss: 0.1550\n",
            "Epoch 168/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 6.5609e-04 - val_loss: 0.1555\n",
            "Epoch 169/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 6.3946e-04 - val_loss: 0.1570\n",
            "Epoch 170/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 6.2518e-04 - val_loss: 0.1577\n",
            "Epoch 171/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 6.1775e-04 - val_loss: 0.1582\n",
            "Epoch 172/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 6.0099e-04 - val_loss: 0.1592\n",
            "Epoch 173/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 5.9641e-04 - val_loss: 0.1591\n",
            "Epoch 174/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 5.8664e-04 - val_loss: 0.1598\n",
            "Epoch 175/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 5.7346e-04 - val_loss: 0.1587\n",
            "Epoch 176/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 5.5576e-04 - val_loss: 0.1592\n",
            "Epoch 177/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 5.5491e-04 - val_loss: 0.1618\n",
            "Epoch 178/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 5.4286e-04 - val_loss: 0.1607\n",
            "Epoch 179/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 5.2556e-04 - val_loss: 0.1617\n",
            "Epoch 180/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 5.1851e-04 - val_loss: 0.1622\n",
            "Epoch 181/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 5.1046e-04 - val_loss: 0.1619\n",
            "Epoch 182/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 4.9941e-04 - val_loss: 0.1625\n",
            "Epoch 183/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 4.8990e-04 - val_loss: 0.1632\n",
            "Epoch 184/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 4.8444e-04 - val_loss: 0.1632\n",
            "Epoch 185/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 4.9331e-04 - val_loss: 0.1632\n",
            "Epoch 186/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 4.6707e-04 - val_loss: 0.1609\n",
            "Epoch 187/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 5.0063e-04 - val_loss: 0.1625\n",
            "Epoch 188/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 4.6326e-04 - val_loss: 0.1647\n",
            "Epoch 189/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 4.4232e-04 - val_loss: 0.1655\n",
            "Epoch 190/600\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 4.3278e-04 - val_loss: 0.1665\n",
            "Epoch 191/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 4.2698e-04 - val_loss: 0.1673\n",
            "Epoch 192/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 4.1668e-04 - val_loss: 0.1672\n",
            "Epoch 193/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 4.1282e-04 - val_loss: 0.1680\n",
            "Epoch 194/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 4.0712e-04 - val_loss: 0.1672\n",
            "Epoch 195/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 3.9763e-04 - val_loss: 0.1678\n",
            "Epoch 196/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 3.9009e-04 - val_loss: 0.1686\n",
            "Epoch 197/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 3.8613e-04 - val_loss: 0.1687\n",
            "Epoch 198/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 3.8352e-04 - val_loss: 0.1689\n",
            "Epoch 199/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 3.7469e-04 - val_loss: 0.1712\n",
            "Epoch 200/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 3.8899e-04 - val_loss: 0.1676\n",
            "Epoch 201/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 3.8723e-04 - val_loss: 0.1687\n",
            "Epoch 202/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 3.7498e-04 - val_loss: 0.1706\n",
            "Epoch 203/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 3.6398e-04 - val_loss: 0.1723\n",
            "Epoch 204/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 3.5084e-04 - val_loss: 0.1721\n",
            "Epoch 205/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 3.4064e-04 - val_loss: 0.1724\n",
            "Epoch 206/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 3.2949e-04 - val_loss: 0.1731\n",
            "Epoch 207/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 3.2424e-04 - val_loss: 0.1748\n",
            "Epoch 208/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 3.1867e-04 - val_loss: 0.1745\n",
            "Epoch 209/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 3.1162e-04 - val_loss: 0.1736\n",
            "Epoch 210/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 3.0855e-04 - val_loss: 0.1746\n",
            "Epoch 211/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 3.0156e-04 - val_loss: 0.1747\n",
            "Epoch 212/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 2.9609e-04 - val_loss: 0.1754\n",
            "Epoch 213/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 2.9239e-04 - val_loss: 0.1763\n",
            "Epoch 214/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 2.8875e-04 - val_loss: 0.1759\n",
            "Epoch 215/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 2.8216e-04 - val_loss: 0.1751\n",
            "Epoch 216/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 2.7955e-04 - val_loss: 0.1751\n",
            "Epoch 217/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 2.7633e-04 - val_loss: 0.1762\n",
            "Epoch 218/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 2.7185e-04 - val_loss: 0.1769\n",
            "Epoch 219/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 2.6832e-04 - val_loss: 0.1777\n",
            "Epoch 220/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 2.6683e-04 - val_loss: 0.1759\n",
            "Epoch 221/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 2.6809e-04 - val_loss: 0.1734\n",
            "Epoch 222/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 2.5584e-04 - val_loss: 0.1742\n",
            "Epoch 223/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 2.5558e-04 - val_loss: 0.1775\n",
            "Epoch 224/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 2.4554e-04 - val_loss: 0.1778\n",
            "Epoch 225/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 2.4366e-04 - val_loss: 0.1782\n",
            "Epoch 226/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 2.3844e-04 - val_loss: 0.1793\n",
            "Epoch 227/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 2.3443e-04 - val_loss: 0.1798\n",
            "Epoch 228/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 2.3236e-04 - val_loss: 0.1797\n",
            "Epoch 229/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 2.2790e-04 - val_loss: 0.1797\n",
            "Epoch 230/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 2.2757e-04 - val_loss: 0.1799\n",
            "Epoch 231/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 2.2118e-04 - val_loss: 0.1812\n",
            "Epoch 232/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 2.1973e-04 - val_loss: 0.1814\n",
            "Epoch 233/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 2.1593e-04 - val_loss: 0.1815\n",
            "Epoch 234/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 2.1293e-04 - val_loss: 0.1820\n",
            "Epoch 235/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 2.1137e-04 - val_loss: 0.1839\n",
            "Epoch 236/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 2.0701e-04 - val_loss: 0.1842\n",
            "Epoch 237/600\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 2.0351e-04 - val_loss: 0.1841\n",
            "Epoch 238/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.9994e-04 - val_loss: 0.1841\n",
            "Epoch 239/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.9792e-04 - val_loss: 0.1845\n",
            "Epoch 240/600\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 2.0153e-04 - val_loss: 0.1870\n",
            "Epoch 241/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.9944e-04 - val_loss: 0.1870\n",
            "Epoch 242/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.9197e-04 - val_loss: 0.1861\n",
            "Epoch 243/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.8710e-04 - val_loss: 0.1868\n",
            "Epoch 244/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.8307e-04 - val_loss: 0.1870\n",
            "Epoch 245/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.8135e-04 - val_loss: 0.1840\n",
            "Epoch 246/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.8056e-04 - val_loss: 0.1849\n",
            "Epoch 247/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.7532e-04 - val_loss: 0.1855\n",
            "Epoch 248/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.7375e-04 - val_loss: 0.1870\n",
            "Epoch 249/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.7174e-04 - val_loss: 0.1875\n",
            "Epoch 250/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.6885e-04 - val_loss: 0.1881\n",
            "Epoch 251/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.6793e-04 - val_loss: 0.1889\n",
            "Epoch 252/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.6442e-04 - val_loss: 0.1894\n",
            "Epoch 253/600\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 1.6224e-04 - val_loss: 0.1894\n",
            "Epoch 254/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.6031e-04 - val_loss: 0.1903\n",
            "Epoch 255/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.5925e-04 - val_loss: 0.1905\n",
            "Epoch 256/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.5678e-04 - val_loss: 0.1902\n",
            "Epoch 257/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.5538e-04 - val_loss: 0.1911\n",
            "Epoch 258/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.5285e-04 - val_loss: 0.1916\n",
            "Epoch 259/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.5064e-04 - val_loss: 0.1917\n",
            "Epoch 260/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.4881e-04 - val_loss: 0.1922\n",
            "Epoch 261/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.4631e-04 - val_loss: 0.1928\n",
            "Epoch 262/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.4513e-04 - val_loss: 0.1931\n",
            "Epoch 263/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.4366e-04 - val_loss: 0.1931\n",
            "Epoch 264/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.4284e-04 - val_loss: 0.1935\n",
            "Epoch 265/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.3722e-04 - val_loss: 0.1956\n",
            "Epoch 266/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.4097e-04 - val_loss: 0.1961\n",
            "Epoch 267/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.3744e-04 - val_loss: 0.1954\n",
            "Epoch 268/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.3439e-04 - val_loss: 0.1947\n",
            "Epoch 269/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.3207e-04 - val_loss: 0.1955\n",
            "Epoch 270/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.3082e-04 - val_loss: 0.1928\n",
            "Epoch 271/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.2950e-04 - val_loss: 0.1931\n",
            "Epoch 272/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.2777e-04 - val_loss: 0.1938\n",
            "Epoch 273/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.2711e-04 - val_loss: 0.1942\n",
            "Epoch 274/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.2437e-04 - val_loss: 0.1950\n",
            "Epoch 275/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.2281e-04 - val_loss: 0.1954\n",
            "Epoch 276/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.2103e-04 - val_loss: 0.1966\n",
            "Epoch 277/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.1942e-04 - val_loss: 0.1969\n",
            "Epoch 278/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.1800e-04 - val_loss: 0.1974\n",
            "Epoch 279/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.1643e-04 - val_loss: 0.1978\n",
            "Epoch 280/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.1554e-04 - val_loss: 0.1981\n",
            "Epoch 281/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.1471e-04 - val_loss: 0.1982\n",
            "Epoch 282/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.1271e-04 - val_loss: 0.1983\n",
            "Epoch 283/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.1135e-04 - val_loss: 0.1994\n",
            "Epoch 284/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.1039e-04 - val_loss: 0.1992\n",
            "Epoch 285/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.0857e-04 - val_loss: 0.1998\n",
            "Epoch 286/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.0713e-04 - val_loss: 0.2000\n",
            "Epoch 287/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.0583e-04 - val_loss: 0.2003\n",
            "Epoch 288/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.0502e-04 - val_loss: 0.1978\n",
            "Epoch 289/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.0421e-04 - val_loss: 0.1983\n",
            "Epoch 290/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.0249e-04 - val_loss: 0.1988\n",
            "Epoch 291/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.0117e-04 - val_loss: 0.1999\n",
            "Epoch 292/600\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 9.9614e-05 - val_loss: 0.2010\n",
            "Epoch 293/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 9.9627e-05 - val_loss: 0.2015\n",
            "Epoch 294/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 9.8803e-05 - val_loss: 0.2015\n",
            "Epoch 295/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 9.9495e-05 - val_loss: 0.2044\n",
            "Epoch 296/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 9.7264e-05 - val_loss: 0.2043\n",
            "Epoch 297/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 9.5933e-05 - val_loss: 0.2033\n",
            "Epoch 298/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 9.6178e-05 - val_loss: 0.2054\n",
            "Epoch 299/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 9.3503e-05 - val_loss: 0.2043\n",
            "Epoch 300/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 9.0566e-05 - val_loss: 0.2049\n",
            "Epoch 301/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 8.9525e-05 - val_loss: 0.2049\n",
            "Epoch 302/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 8.8809e-05 - val_loss: 0.2054\n",
            "Epoch 303/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 8.7497e-05 - val_loss: 0.2054\n",
            "Epoch 304/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 8.6238e-05 - val_loss: 0.2059\n",
            "Epoch 305/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 8.5581e-05 - val_loss: 0.2065\n",
            "Epoch 306/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 8.4553e-05 - val_loss: 0.2062\n",
            "Epoch 307/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 8.3745e-05 - val_loss: 0.2073\n",
            "Epoch 308/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 8.2346e-05 - val_loss: 0.2074\n",
            "Epoch 309/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 8.1399e-05 - val_loss: 0.2082\n",
            "Epoch 310/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 8.0421e-05 - val_loss: 0.2079\n",
            "Epoch 311/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 7.9473e-05 - val_loss: 0.2085\n",
            "Epoch 312/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 7.8690e-05 - val_loss: 0.2094\n",
            "Epoch 313/600\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 7.8342e-05 - val_loss: 0.2096\n",
            "Epoch 314/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 7.9329e-05 - val_loss: 0.2066\n",
            "Epoch 315/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 8.1094e-05 - val_loss: 0.2077\n",
            "Epoch 316/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 7.9248e-05 - val_loss: 0.2090\n",
            "Epoch 317/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 7.7375e-05 - val_loss: 0.2099\n",
            "Epoch 318/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 7.5104e-05 - val_loss: 0.2107\n",
            "Epoch 319/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 7.4023e-05 - val_loss: 0.2115\n",
            "Epoch 320/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 7.1892e-05 - val_loss: 0.2116\n",
            "Epoch 321/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 7.0909e-05 - val_loss: 0.2117\n",
            "Epoch 322/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 6.9664e-05 - val_loss: 0.2120\n",
            "Epoch 323/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 6.8891e-05 - val_loss: 0.2109\n",
            "Epoch 324/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 6.8193e-05 - val_loss: 0.2126\n",
            "Epoch 325/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 6.6996e-05 - val_loss: 0.2135\n",
            "Epoch 326/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 6.6464e-05 - val_loss: 0.2137\n",
            "Epoch 327/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 6.5899e-05 - val_loss: 0.2145\n",
            "Epoch 328/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 6.4301e-05 - val_loss: 0.2144\n",
            "Epoch 329/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 6.3904e-05 - val_loss: 0.2147\n",
            "Epoch 330/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 6.2949e-05 - val_loss: 0.2154\n",
            "Epoch 331/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 6.3016e-05 - val_loss: 0.2160\n",
            "Epoch 332/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 6.1934e-05 - val_loss: 0.2149\n",
            "Epoch 333/600\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 6.1473e-05 - val_loss: 0.2160\n",
            "Epoch 334/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 6.0094e-05 - val_loss: 0.2161\n",
            "Epoch 335/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 5.9415e-05 - val_loss: 0.2162\n",
            "Epoch 336/600\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 5.9149e-05 - val_loss: 0.2167\n",
            "Epoch 337/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 5.8352e-05 - val_loss: 0.2170\n",
            "Epoch 338/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 5.8008e-05 - val_loss: 0.2173\n",
            "Epoch 339/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 5.7116e-05 - val_loss: 0.2172\n",
            "Epoch 340/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 5.6258e-05 - val_loss: 0.2173\n",
            "Epoch 341/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 5.5724e-05 - val_loss: 0.2179\n",
            "Epoch 342/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 5.5725e-05 - val_loss: 0.2186\n",
            "Epoch 343/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 5.5811e-05 - val_loss: 0.2189\n",
            "Epoch 344/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 5.4404e-05 - val_loss: 0.2187\n",
            "Epoch 345/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 5.3320e-05 - val_loss: 0.2201\n",
            "Epoch 346/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 5.2832e-05 - val_loss: 0.2201\n",
            "Epoch 347/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 5.2225e-05 - val_loss: 0.2201\n",
            "Epoch 348/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 5.1739e-05 - val_loss: 0.2204\n",
            "Epoch 349/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 5.1107e-05 - val_loss: 0.2204\n",
            "Epoch 350/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 5.0707e-05 - val_loss: 0.2210\n",
            "Epoch 351/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 4.9761e-05 - val_loss: 0.2212\n",
            "Epoch 352/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 4.9573e-05 - val_loss: 0.2208\n",
            "Epoch 353/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 4.9184e-05 - val_loss: 0.2205\n",
            "Epoch 354/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 4.8266e-05 - val_loss: 0.2218\n",
            "Epoch 355/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 4.8098e-05 - val_loss: 0.2217\n",
            "Epoch 356/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 4.7307e-05 - val_loss: 0.2223\n",
            "Epoch 357/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 4.6915e-05 - val_loss: 0.2225\n",
            "Epoch 358/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 4.6336e-05 - val_loss: 0.2236\n",
            "Epoch 359/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 4.5868e-05 - val_loss: 0.2235\n",
            "Epoch 360/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 4.5506e-05 - val_loss: 0.2235\n",
            "Epoch 361/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 4.4970e-05 - val_loss: 0.2234\n",
            "Epoch 362/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 4.4397e-05 - val_loss: 0.2252\n",
            "Epoch 363/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 4.4016e-05 - val_loss: 0.2253\n",
            "Epoch 364/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 4.3461e-05 - val_loss: 0.2253\n",
            "Epoch 365/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 4.3184e-05 - val_loss: 0.2259\n",
            "Epoch 366/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 4.3027e-05 - val_loss: 0.2262\n",
            "Epoch 367/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 4.2921e-05 - val_loss: 0.2253\n",
            "Epoch 368/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 4.2606e-05 - val_loss: 0.2263\n",
            "Epoch 369/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 4.1942e-05 - val_loss: 0.2271\n",
            "Epoch 370/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 4.0737e-05 - val_loss: 0.2273\n",
            "Epoch 371/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 4.0198e-05 - val_loss: 0.2283\n",
            "Epoch 372/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 3.9618e-05 - val_loss: 0.2292\n",
            "Epoch 373/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 3.9173e-05 - val_loss: 0.2290\n",
            "Epoch 374/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 3.8722e-05 - val_loss: 0.2290\n",
            "Epoch 375/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 3.8446e-05 - val_loss: 0.2285\n",
            "Epoch 376/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 3.7808e-05 - val_loss: 0.2293\n",
            "Epoch 377/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 3.7561e-05 - val_loss: 0.2299\n",
            "Epoch 378/600\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 3.7062e-05 - val_loss: 0.2294\n",
            "Epoch 379/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 3.7191e-05 - val_loss: 0.2285\n",
            "Epoch 380/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 3.6790e-05 - val_loss: 0.2293\n",
            "Epoch 381/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 3.6306e-05 - val_loss: 0.2297\n",
            "Epoch 382/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 3.5486e-05 - val_loss: 0.2309\n",
            "Epoch 383/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 3.5203e-05 - val_loss: 0.2316\n",
            "Epoch 384/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 3.5092e-05 - val_loss: 0.2311\n",
            "Epoch 385/600\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 3.4608e-05 - val_loss: 0.2317\n",
            "Epoch 386/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 3.4218e-05 - val_loss: 0.2318\n",
            "Epoch 387/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 3.3597e-05 - val_loss: 0.2318\n",
            "Epoch 388/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 3.3479e-05 - val_loss: 0.2326\n",
            "Epoch 389/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 3.2914e-05 - val_loss: 0.2323\n",
            "Epoch 390/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 3.2750e-05 - val_loss: 0.2325\n",
            "Epoch 391/600\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 3.2307e-05 - val_loss: 0.2333\n",
            "Epoch 392/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 3.2251e-05 - val_loss: 0.2339\n",
            "Epoch 393/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 3.1797e-05 - val_loss: 0.2331\n",
            "Epoch 394/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 3.1623e-05 - val_loss: 0.2330\n",
            "Epoch 395/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 3.1011e-05 - val_loss: 0.2345\n",
            "Epoch 396/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 3.0810e-05 - val_loss: 0.2351\n",
            "Epoch 397/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 3.1237e-05 - val_loss: 0.2342\n",
            "Epoch 398/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 3.1003e-05 - val_loss: 0.2345\n",
            "Epoch 399/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 3.0653e-05 - val_loss: 0.2347\n",
            "Epoch 400/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 3.0165e-05 - val_loss: 0.2353\n",
            "Epoch 401/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 2.9816e-05 - val_loss: 0.2359\n",
            "Epoch 402/600\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 2.9144e-05 - val_loss: 0.2357\n",
            "Epoch 403/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 2.8882e-05 - val_loss: 0.2371\n",
            "Epoch 404/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 2.8346e-05 - val_loss: 0.2369\n",
            "Epoch 405/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 2.7993e-05 - val_loss: 0.2367\n",
            "Epoch 406/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 2.7776e-05 - val_loss: 0.2373\n",
            "Epoch 407/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 2.7432e-05 - val_loss: 0.2370\n",
            "Epoch 408/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 2.7073e-05 - val_loss: 0.2375\n",
            "Epoch 409/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 2.6729e-05 - val_loss: 0.2379\n",
            "Epoch 410/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 2.6588e-05 - val_loss: 0.2384\n",
            "Epoch 411/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 2.6462e-05 - val_loss: 0.2389\n",
            "Epoch 412/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 2.6344e-05 - val_loss: 0.2383\n",
            "Epoch 413/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 2.5702e-05 - val_loss: 0.2392\n",
            "Epoch 414/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 2.5400e-05 - val_loss: 0.2394\n",
            "Epoch 415/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 2.5082e-05 - val_loss: 0.2405\n",
            "Epoch 416/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 2.5024e-05 - val_loss: 0.2401\n",
            "Epoch 417/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 2.4753e-05 - val_loss: 0.2406\n",
            "Epoch 418/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 2.4432e-05 - val_loss: 0.2407\n",
            "Epoch 419/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 2.4283e-05 - val_loss: 0.2408\n",
            "Epoch 420/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 2.3946e-05 - val_loss: 0.2416\n",
            "Epoch 421/600\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 2.3744e-05 - val_loss: 0.2416\n",
            "Epoch 422/600\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 2.3620e-05 - val_loss: 0.2419\n",
            "Epoch 423/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 2.3320e-05 - val_loss: 0.2422\n",
            "Epoch 424/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 2.3125e-05 - val_loss: 0.2425\n",
            "Epoch 425/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 2.2949e-05 - val_loss: 0.2437\n",
            "Epoch 426/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 2.2570e-05 - val_loss: 0.2443\n",
            "Epoch 427/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 2.2469e-05 - val_loss: 0.2436\n",
            "Epoch 428/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 2.2150e-05 - val_loss: 0.2438\n",
            "Epoch 429/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 2.1959e-05 - val_loss: 0.2441\n",
            "Epoch 430/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 2.1632e-05 - val_loss: 0.2431\n",
            "Epoch 431/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 2.1752e-05 - val_loss: 0.2435\n",
            "Epoch 432/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 2.1350e-05 - val_loss: 0.2451\n",
            "Epoch 433/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 2.1038e-05 - val_loss: 0.2449\n",
            "Epoch 434/600\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 2.0785e-05 - val_loss: 0.2446\n",
            "Epoch 435/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 2.0552e-05 - val_loss: 0.2453\n",
            "Epoch 436/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 2.0332e-05 - val_loss: 0.2467\n",
            "Epoch 437/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 2.0144e-05 - val_loss: 0.2473\n",
            "Epoch 438/600\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 1.9960e-05 - val_loss: 0.2476\n",
            "Epoch 439/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.9663e-05 - val_loss: 0.2481\n",
            "Epoch 440/600\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 1.9758e-05 - val_loss: 0.2486\n",
            "Epoch 441/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.9492e-05 - val_loss: 0.2479\n",
            "Epoch 442/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.9192e-05 - val_loss: 0.2483\n",
            "Epoch 443/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.9028e-05 - val_loss: 0.2491\n",
            "Epoch 444/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.8743e-05 - val_loss: 0.2496\n",
            "Epoch 445/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.8581e-05 - val_loss: 0.2494\n",
            "Epoch 446/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.8381e-05 - val_loss: 0.2493\n",
            "Epoch 447/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.8207e-05 - val_loss: 0.2502\n",
            "Epoch 448/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.8204e-05 - val_loss: 0.2502\n",
            "Epoch 449/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.7875e-05 - val_loss: 0.2498\n",
            "Epoch 450/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.7783e-05 - val_loss: 0.2493\n",
            "Epoch 451/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.7600e-05 - val_loss: 0.2504\n",
            "Epoch 452/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.7463e-05 - val_loss: 0.2502\n",
            "Epoch 453/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.7271e-05 - val_loss: 0.2504\n",
            "Epoch 454/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.6990e-05 - val_loss: 0.2513\n",
            "Epoch 455/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.6952e-05 - val_loss: 0.2517\n",
            "Epoch 456/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.6765e-05 - val_loss: 0.2519\n",
            "Epoch 457/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.6613e-05 - val_loss: 0.2515\n",
            "Epoch 458/600\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 1.6427e-05 - val_loss: 0.2519\n",
            "Epoch 459/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.6327e-05 - val_loss: 0.2513\n",
            "Epoch 460/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.6203e-05 - val_loss: 0.2525\n",
            "Epoch 461/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.5858e-05 - val_loss: 0.2531\n",
            "Epoch 462/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.5739e-05 - val_loss: 0.2535\n",
            "Epoch 463/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.5630e-05 - val_loss: 0.2536\n",
            "Epoch 464/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.5488e-05 - val_loss: 0.2541\n",
            "Epoch 465/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.5310e-05 - val_loss: 0.2542\n",
            "Epoch 466/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.5249e-05 - val_loss: 0.2538\n",
            "Epoch 467/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.5097e-05 - val_loss: 0.2540\n",
            "Epoch 468/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.4872e-05 - val_loss: 0.2550\n",
            "Epoch 469/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.4781e-05 - val_loss: 0.2547\n",
            "Epoch 470/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.4639e-05 - val_loss: 0.2554\n",
            "Epoch 471/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.4502e-05 - val_loss: 0.2557\n",
            "Epoch 472/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.4455e-05 - val_loss: 0.2561\n",
            "Epoch 473/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.4346e-05 - val_loss: 0.2563\n",
            "Epoch 474/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.4177e-05 - val_loss: 0.2569\n",
            "Epoch 475/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.3928e-05 - val_loss: 0.2569\n",
            "Epoch 476/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.3894e-05 - val_loss: 0.2567\n",
            "Epoch 477/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.3686e-05 - val_loss: 0.2573\n",
            "Epoch 478/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.3530e-05 - val_loss: 0.2572\n",
            "Epoch 479/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.3434e-05 - val_loss: 0.2577\n",
            "Epoch 480/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.3294e-05 - val_loss: 0.2584\n",
            "Epoch 481/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.3143e-05 - val_loss: 0.2579\n",
            "Epoch 482/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.3161e-05 - val_loss: 0.2588\n",
            "Epoch 483/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.2919e-05 - val_loss: 0.2592\n",
            "Epoch 484/600\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 1.2909e-05 - val_loss: 0.2557\n",
            "Epoch 485/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.2702e-05 - val_loss: 0.2561\n",
            "Epoch 486/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.2550e-05 - val_loss: 0.2568\n",
            "Epoch 487/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.2473e-05 - val_loss: 0.2578\n",
            "Epoch 488/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.2327e-05 - val_loss: 0.2592\n",
            "Epoch 489/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.2197e-05 - val_loss: 0.2602\n",
            "Epoch 490/600\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 1.2078e-05 - val_loss: 0.2600\n",
            "Epoch 491/600\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 1.1929e-05 - val_loss: 0.2610\n",
            "Epoch 492/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.1780e-05 - val_loss: 0.2614\n",
            "Epoch 493/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.1777e-05 - val_loss: 0.2617\n",
            "Epoch 494/600\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 1.1619e-05 - val_loss: 0.2619\n",
            "Epoch 495/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.1490e-05 - val_loss: 0.2619\n",
            "Epoch 496/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.1363e-05 - val_loss: 0.2617\n",
            "Epoch 497/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.1436e-05 - val_loss: 0.2620\n",
            "Epoch 498/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.1252e-05 - val_loss: 0.2629\n",
            "Epoch 499/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.1091e-05 - val_loss: 0.2629\n",
            "Epoch 500/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.0868e-05 - val_loss: 0.2634\n",
            "Epoch 501/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.0806e-05 - val_loss: 0.2638\n",
            "Epoch 502/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.0766e-05 - val_loss: 0.2636\n",
            "Epoch 503/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.0596e-05 - val_loss: 0.2644\n",
            "Epoch 504/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.0521e-05 - val_loss: 0.2643\n",
            "Epoch 505/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.0359e-05 - val_loss: 0.2645\n",
            "Epoch 506/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.0402e-05 - val_loss: 0.2621\n",
            "Epoch 507/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.0294e-05 - val_loss: 0.2626\n",
            "Epoch 508/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.0094e-05 - val_loss: 0.2634\n",
            "Epoch 509/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.0044e-05 - val_loss: 0.2641\n",
            "Epoch 510/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.0026e-05 - val_loss: 0.2632\n",
            "Epoch 511/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.0135e-05 - val_loss: 0.2639\n",
            "Epoch 512/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 9.9845e-06 - val_loss: 0.2645\n",
            "Epoch 513/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 9.8531e-06 - val_loss: 0.2666\n",
            "Epoch 514/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 9.6195e-06 - val_loss: 0.2667\n",
            "Epoch 515/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 9.4925e-06 - val_loss: 0.2670\n",
            "Epoch 516/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 9.3326e-06 - val_loss: 0.2671\n",
            "Epoch 517/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 9.2040e-06 - val_loss: 0.2675\n",
            "Epoch 518/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 9.1379e-06 - val_loss: 0.2678\n",
            "Epoch 519/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 8.9997e-06 - val_loss: 0.2681\n",
            "Epoch 520/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 8.9576e-06 - val_loss: 0.2683\n",
            "Epoch 521/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 8.8152e-06 - val_loss: 0.2685\n",
            "Epoch 522/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 8.7592e-06 - val_loss: 0.2685\n",
            "Epoch 523/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 8.6763e-06 - val_loss: 0.2688\n",
            "Epoch 524/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 8.6479e-06 - val_loss: 0.2697\n",
            "Epoch 525/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 8.5499e-06 - val_loss: 0.2697\n",
            "Epoch 526/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 8.4259e-06 - val_loss: 0.2696\n",
            "Epoch 527/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 8.3742e-06 - val_loss: 0.2691\n",
            "Epoch 528/600\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 8.2743e-06 - val_loss: 0.2697\n",
            "Epoch 529/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 8.2030e-06 - val_loss: 0.2705\n",
            "Epoch 530/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 8.1322e-06 - val_loss: 0.2707\n",
            "Epoch 531/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 8.0756e-06 - val_loss: 0.2708\n",
            "Epoch 532/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 7.9571e-06 - val_loss: 0.2711\n",
            "Epoch 533/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 7.9210e-06 - val_loss: 0.2716\n",
            "Epoch 534/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 7.9279e-06 - val_loss: 0.2726\n",
            "Epoch 535/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 7.8241e-06 - val_loss: 0.2723\n",
            "Epoch 536/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 7.7333e-06 - val_loss: 0.2725\n",
            "Epoch 537/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 7.5021e-06 - val_loss: 0.2709\n",
            "Epoch 538/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 7.5992e-06 - val_loss: 0.2712\n",
            "Epoch 539/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 7.5064e-06 - val_loss: 0.2728\n",
            "Epoch 540/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 7.4724e-06 - val_loss: 0.2739\n",
            "Epoch 541/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 7.3761e-06 - val_loss: 0.2743\n",
            "Epoch 542/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 7.2969e-06 - val_loss: 0.2744\n",
            "Epoch 543/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 7.1869e-06 - val_loss: 0.2745\n",
            "Epoch 544/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 7.0980e-06 - val_loss: 0.2756\n",
            "Epoch 545/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 7.0772e-06 - val_loss: 0.2747\n",
            "Epoch 546/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 6.9648e-06 - val_loss: 0.2751\n",
            "Epoch 547/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 6.9166e-06 - val_loss: 0.2760\n",
            "Epoch 548/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 6.8350e-06 - val_loss: 0.2759\n",
            "Epoch 549/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 6.8200e-06 - val_loss: 0.2772\n",
            "Epoch 550/600\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 6.8055e-06 - val_loss: 0.2764\n",
            "Epoch 551/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 6.6882e-06 - val_loss: 0.2768\n",
            "Epoch 552/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 6.6307e-06 - val_loss: 0.2771\n",
            "Epoch 553/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 6.5499e-06 - val_loss: 0.2772\n",
            "Epoch 554/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 6.4815e-06 - val_loss: 0.2773\n",
            "Epoch 555/600\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 6.4341e-06 - val_loss: 0.2769\n",
            "Epoch 556/600\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 6.3543e-06 - val_loss: 0.2777\n",
            "Epoch 557/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 6.2938e-06 - val_loss: 0.2780\n",
            "Epoch 558/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 6.2522e-06 - val_loss: 0.2781\n",
            "Epoch 559/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 6.1866e-06 - val_loss: 0.2782\n",
            "Epoch 560/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 6.3095e-06 - val_loss: 0.2772\n",
            "Epoch 561/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 6.1868e-06 - val_loss: 0.2787\n",
            "Epoch 562/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 6.2072e-06 - val_loss: 0.2795\n",
            "Epoch 563/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 6.1142e-06 - val_loss: 0.2801\n",
            "Epoch 564/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 5.9923e-06 - val_loss: 0.2801\n",
            "Epoch 565/600\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 5.9459e-06 - val_loss: 0.2797\n",
            "Epoch 566/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 5.8296e-06 - val_loss: 0.2803\n",
            "Epoch 567/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 5.7351e-06 - val_loss: 0.2809\n",
            "Epoch 568/600\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 5.7176e-06 - val_loss: 0.2808\n",
            "Epoch 569/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 5.6212e-06 - val_loss: 0.2815\n",
            "Epoch 570/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 5.5800e-06 - val_loss: 0.2811\n",
            "Epoch 571/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 5.5385e-06 - val_loss: 0.2813\n",
            "Epoch 572/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 5.4390e-06 - val_loss: 0.2821\n",
            "Epoch 573/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 5.4110e-06 - val_loss: 0.2822\n",
            "Epoch 574/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 5.4202e-06 - val_loss: 0.2806\n",
            "Epoch 575/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 5.3545e-06 - val_loss: 0.2812\n",
            "Epoch 576/600\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 5.2868e-06 - val_loss: 0.2824\n",
            "Epoch 577/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 5.2300e-06 - val_loss: 0.2829\n",
            "Epoch 578/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 5.1972e-06 - val_loss: 0.2824\n",
            "Epoch 579/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 5.1432e-06 - val_loss: 0.2831\n",
            "Epoch 580/600\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 5.0757e-06 - val_loss: 0.2838\n",
            "Epoch 581/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 5.0252e-06 - val_loss: 0.2835\n",
            "Epoch 582/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 5.0098e-06 - val_loss: 0.2838\n",
            "Epoch 583/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 4.9298e-06 - val_loss: 0.2846\n",
            "Epoch 584/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 4.9114e-06 - val_loss: 0.2860\n",
            "Epoch 585/600\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 4.9226e-06 - val_loss: 0.2864\n",
            "Epoch 586/600\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 4.8441e-06 - val_loss: 0.2857\n",
            "Epoch 587/600\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 4.7821e-06 - val_loss: 0.2850\n",
            "Epoch 588/600\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 4.7166e-06 - val_loss: 0.2857\n",
            "Epoch 589/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 4.6782e-06 - val_loss: 0.2859\n",
            "Epoch 590/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 4.6427e-06 - val_loss: 0.2867\n",
            "Epoch 591/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 4.5950e-06 - val_loss: 0.2866\n",
            "Epoch 592/600\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 4.5620e-06 - val_loss: 0.2869\n",
            "Epoch 593/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 4.5049e-06 - val_loss: 0.2869\n",
            "Epoch 594/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 4.4667e-06 - val_loss: 0.2871\n",
            "Epoch 595/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 4.4191e-06 - val_loss: 0.2879\n",
            "Epoch 596/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 4.3944e-06 - val_loss: 0.2875\n",
            "Epoch 597/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 4.3535e-06 - val_loss: 0.2887\n",
            "Epoch 598/600\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 4.3326e-06 - val_loss: 0.2893\n",
            "Epoch 599/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 4.2716e-06 - val_loss: 0.2893\n",
            "Epoch 600/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 4.2467e-06 - val_loss: 0.2892\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4a301f87f0>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "#Step1 -: initialising the model\n",
        "ann = Sequential()\n",
        "\n",
        "#step2-: add the layers...\n",
        "\n",
        "# 30 because at first no. of cols I have are 30\n",
        "ann.add(Dense(units=30, activation=\"relu\"))\n",
        "# \n",
        "ann.add(Dense(units=15, activation=\"relu\"))\n",
        "ann.add(Dense(units=1, activation=\"sigmoid\"))\n",
        "\n",
        "#step3 -: establish the connection between the layers\n",
        "ann.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\n",
        "\n",
        "#step4-: train the model\n",
        "ann.fit(xtrain, ytrain, epochs=600, validation_data=(xtest, ytest))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ann.history.history"
      ],
      "metadata": {
        "id": "bLBz-RHjW_Qe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8be302e7-c57c-49b6-e4b4-6d284d614ee3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'loss': [0.6225557327270508,\n",
              "  0.4243448078632355,\n",
              "  0.30319884419441223,\n",
              "  0.22340847551822662,\n",
              "  0.17364652454853058,\n",
              "  0.14137817919254303,\n",
              "  0.12013240158557892,\n",
              "  0.10452958196401596,\n",
              "  0.09362521022558212,\n",
              "  0.08500030636787415,\n",
              "  0.07882120460271835,\n",
              "  0.0733586996793747,\n",
              "  0.06897297501564026,\n",
              "  0.06485839188098907,\n",
              "  0.06130527704954147,\n",
              "  0.05840418115258217,\n",
              "  0.05572763830423355,\n",
              "  0.05301395431160927,\n",
              "  0.05097611993551254,\n",
              "  0.04876723140478134,\n",
              "  0.04683913663029671,\n",
              "  0.044661011546850204,\n",
              "  0.04294069856405258,\n",
              "  0.041535213589668274,\n",
              "  0.03998193144798279,\n",
              "  0.03767611086368561,\n",
              "  0.036483217030763626,\n",
              "  0.03532516956329346,\n",
              "  0.03398679196834564,\n",
              "  0.032293934375047684,\n",
              "  0.03142668679356575,\n",
              "  0.02991166152060032,\n",
              "  0.029069550335407257,\n",
              "  0.027881549671292305,\n",
              "  0.027211152017116547,\n",
              "  0.026134680956602097,\n",
              "  0.025228355079889297,\n",
              "  0.024242516607046127,\n",
              "  0.023530293256044388,\n",
              "  0.022906463593244553,\n",
              "  0.021724512800574303,\n",
              "  0.0211263969540596,\n",
              "  0.02032618410885334,\n",
              "  0.019644705578684807,\n",
              "  0.018988940864801407,\n",
              "  0.018423428758978844,\n",
              "  0.017723193392157555,\n",
              "  0.017282208427786827,\n",
              "  0.01674819178879261,\n",
              "  0.016617637127637863,\n",
              "  0.01563868299126625,\n",
              "  0.015110095962882042,\n",
              "  0.01459688413888216,\n",
              "  0.014014695771038532,\n",
              "  0.013654564507305622,\n",
              "  0.013367521576583385,\n",
              "  0.0127483531832695,\n",
              "  0.013289465568959713,\n",
              "  0.012024173513054848,\n",
              "  0.011634636670351028,\n",
              "  0.01115025207400322,\n",
              "  0.01080748438835144,\n",
              "  0.010647483170032501,\n",
              "  0.010267558507621288,\n",
              "  0.009916278533637524,\n",
              "  0.009772061370313168,\n",
              "  0.00941295176744461,\n",
              "  0.00895580742508173,\n",
              "  0.008861580863595009,\n",
              "  0.008400694467127323,\n",
              "  0.008481672033667564,\n",
              "  0.008068978786468506,\n",
              "  0.007673262152820826,\n",
              "  0.007283804006874561,\n",
              "  0.007133678067475557,\n",
              "  0.006863353308290243,\n",
              "  0.006720046512782574,\n",
              "  0.006466316990554333,\n",
              "  0.006336605176329613,\n",
              "  0.006179594900459051,\n",
              "  0.005941353738307953,\n",
              "  0.005726515781134367,\n",
              "  0.005625072401016951,\n",
              "  0.005564672872424126,\n",
              "  0.005344310309737921,\n",
              "  0.005079507362097502,\n",
              "  0.00497803371399641,\n",
              "  0.0048407600261271,\n",
              "  0.004646993242204189,\n",
              "  0.004511644598096609,\n",
              "  0.0043763513676822186,\n",
              "  0.0042046173475682735,\n",
              "  0.004124014172703028,\n",
              "  0.004046059213578701,\n",
              "  0.003865638514980674,\n",
              "  0.0038084755651652813,\n",
              "  0.003677659435197711,\n",
              "  0.0036009105388075113,\n",
              "  0.004102686885744333,\n",
              "  0.003386687720194459,\n",
              "  0.0032773755956441164,\n",
              "  0.0032023387029767036,\n",
              "  0.0031006967183202505,\n",
              "  0.0030606607906520367,\n",
              "  0.0030198406893759966,\n",
              "  0.0028501367196440697,\n",
              "  0.00279573374427855,\n",
              "  0.002727038227021694,\n",
              "  0.002712046727538109,\n",
              "  0.0026371204294264317,\n",
              "  0.0025231840554624796,\n",
              "  0.002417313400655985,\n",
              "  0.002374341245740652,\n",
              "  0.0023047837894409895,\n",
              "  0.002237604930996895,\n",
              "  0.0023034107871353626,\n",
              "  0.0021613328717648983,\n",
              "  0.0022017760202288628,\n",
              "  0.002123531885445118,\n",
              "  0.0019531678408384323,\n",
              "  0.0018868971383199096,\n",
              "  0.0018298858776688576,\n",
              "  0.0018596441950649023,\n",
              "  0.001742384280078113,\n",
              "  0.0017104477155953646,\n",
              "  0.0016722599975764751,\n",
              "  0.0016473607392981648,\n",
              "  0.001591821201145649,\n",
              "  0.0015419712290167809,\n",
              "  0.0015329447342082858,\n",
              "  0.00149105628952384,\n",
              "  0.0014686371432617307,\n",
              "  0.0014230581000447273,\n",
              "  0.001391617814078927,\n",
              "  0.0013563318643718958,\n",
              "  0.0013257160317152739,\n",
              "  0.0013031538110226393,\n",
              "  0.001252577407285571,\n",
              "  0.0012482238234952092,\n",
              "  0.0012369300238788128,\n",
              "  0.001205257372930646,\n",
              "  0.0012012884253636003,\n",
              "  0.001129124197177589,\n",
              "  0.0010965595720335841,\n",
              "  0.0010777013376355171,\n",
              "  0.001049351179972291,\n",
              "  0.0010327991330996156,\n",
              "  0.0010110463481396437,\n",
              "  0.0009910034714266658,\n",
              "  0.0009790741605684161,\n",
              "  0.0009539591264910996,\n",
              "  0.000944663246627897,\n",
              "  0.000918620906304568,\n",
              "  0.0008940738625824451,\n",
              "  0.0008660765015520155,\n",
              "  0.0008555134991183877,\n",
              "  0.0008382608066312969,\n",
              "  0.000818538770545274,\n",
              "  0.0008015664643608034,\n",
              "  0.0007824261556379497,\n",
              "  0.0007697539404034615,\n",
              "  0.000765608623623848,\n",
              "  0.0007637263042852283,\n",
              "  0.0007298802374862134,\n",
              "  0.000706356018781662,\n",
              "  0.0006775902002118528,\n",
              "  0.0006692747701890767,\n",
              "  0.0006560860201716423,\n",
              "  0.0006394619704224169,\n",
              "  0.0006251849699765444,\n",
              "  0.0006177508039399981,\n",
              "  0.0006009904318489134,\n",
              "  0.0005964061128906906,\n",
              "  0.0005866402643732727,\n",
              "  0.000573461118619889,\n",
              "  0.0005557608092203736,\n",
              "  0.0005549099296331406,\n",
              "  0.0005428591393865645,\n",
              "  0.0005255645955912769,\n",
              "  0.0005185085465200245,\n",
              "  0.0005104626179672778,\n",
              "  0.00049940514145419,\n",
              "  0.0004898998304270208,\n",
              "  0.00048443509149365127,\n",
              "  0.000493308063596487,\n",
              "  0.00046706615830771625,\n",
              "  0.0005006291321478784,\n",
              "  0.00046325786388479173,\n",
              "  0.0004423185018822551,\n",
              "  0.00043278277735225856,\n",
              "  0.0004269752826076001,\n",
              "  0.0004166842554695904,\n",
              "  0.0004128177824895829,\n",
              "  0.0004071247240062803,\n",
              "  0.00039762843516655266,\n",
              "  0.0003900948795489967,\n",
              "  0.0003861347504425794,\n",
              "  0.0003835170064121485,\n",
              "  0.0003746872826013714,\n",
              "  0.00038899434730410576,\n",
              "  0.0003872285597026348,\n",
              "  0.0003749821917153895,\n",
              "  0.0003639831265900284,\n",
              "  0.00035083669354207814,\n",
              "  0.0003406365867704153,\n",
              "  0.00032948824809864163,\n",
              "  0.00032424420351162553,\n",
              "  0.00031866744393482804,\n",
              "  0.0003116168372798711,\n",
              "  0.0003085477801505476,\n",
              "  0.00030155840795487165,\n",
              "  0.00029608752811327577,\n",
              "  0.00029239204013720155,\n",
              "  0.0002887513255700469,\n",
              "  0.00028215948259457946,\n",
              "  0.0002795535256154835,\n",
              "  0.00027633376885205507,\n",
              "  0.00027185474755242467,\n",
              "  0.00026832116418518126,\n",
              "  0.00026683302712626755,\n",
              "  0.00026808815891854465,\n",
              "  0.00025583626120351255,\n",
              "  0.00025558064226061106,\n",
              "  0.00024554465198889375,\n",
              "  0.0002436642680549994,\n",
              "  0.000238440785324201,\n",
              "  0.00023443384270649403,\n",
              "  0.00023236066044773906,\n",
              "  0.00022789693321101367,\n",
              "  0.00022757389524485916,\n",
              "  0.00022117827029433101,\n",
              "  0.00021972782269585878,\n",
              "  0.0002159286814276129,\n",
              "  0.0002129261993104592,\n",
              "  0.0002113713853759691,\n",
              "  0.00020700985623989254,\n",
              "  0.00020351402054075152,\n",
              "  0.00019994292233604938,\n",
              "  0.00019792235980276018,\n",
              "  0.00020152963406872004,\n",
              "  0.0001994394406210631,\n",
              "  0.00019196666835341603,\n",
              "  0.00018709588039200753,\n",
              "  0.0001830653491197154,\n",
              "  0.0001813490380300209,\n",
              "  0.00018056080443784595,\n",
              "  0.000175319000845775,\n",
              "  0.00017374588060192764,\n",
              "  0.00017174132517538965,\n",
              "  0.0001688480406301096,\n",
              "  0.0001679313718341291,\n",
              "  0.00016441612388007343,\n",
              "  0.00016224253340624273,\n",
              "  0.00016030629922170192,\n",
              "  0.00015925434126984328,\n",
              "  0.00015677795454394072,\n",
              "  0.00015538053412456065,\n",
              "  0.0001528508000774309,\n",
              "  0.00015063687169458717,\n",
              "  0.0001488060806877911,\n",
              "  0.0001463070948375389,\n",
              "  0.00014513373025693,\n",
              "  0.00014365608512889594,\n",
              "  0.00014283978089224547,\n",
              "  0.00013721697905566543,\n",
              "  0.0001409652759321034,\n",
              "  0.0001374354906147346,\n",
              "  0.0001343904878012836,\n",
              "  0.0001320748997386545,\n",
              "  0.00013081553333904594,\n",
              "  0.00012950311065651476,\n",
              "  0.00012777077790815383,\n",
              "  0.00012711039744317532,\n",
              "  0.00012436516408342868,\n",
              "  0.0001228121982421726,\n",
              "  0.00012102662731194869,\n",
              "  0.00011941833508899435,\n",
              "  0.00011800396896433085,\n",
              "  0.00011643048492260277,\n",
              "  0.00011554281081771478,\n",
              "  0.00011470539175206795,\n",
              "  0.00011271449329797179,\n",
              "  0.0001113513862947002,\n",
              "  0.00011038786760764197,\n",
              "  0.00010857273446163163,\n",
              "  0.00010712866787798703,\n",
              "  0.00010583410767139867,\n",
              "  0.00010501623182790354,\n",
              "  0.00010420685430290177,\n",
              "  0.00010249009937979281,\n",
              "  0.00010116997145814821,\n",
              "  9.96144735836424e-05,\n",
              "  9.962738113244995e-05,\n",
              "  9.880324796540663e-05,\n",
              "  9.94951042230241e-05,\n",
              "  9.726427379064262e-05,\n",
              "  9.593323920853436e-05,\n",
              "  9.617771138437092e-05,\n",
              "  9.350338950753212e-05,\n",
              "  9.05659981071949e-05,\n",
              "  8.952486678026617e-05,\n",
              "  8.880870882421732e-05,\n",
              "  8.749669359531254e-05,\n",
              "  8.623809117125347e-05,\n",
              "  8.558064291719347e-05,\n",
              "  8.45525210024789e-05,\n",
              "  8.374545723199844e-05,\n",
              "  8.234557026298717e-05,\n",
              "  8.139859710354358e-05,\n",
              "  8.042083209147677e-05,\n",
              "  7.94727384345606e-05,\n",
              "  7.869007822591811e-05,\n",
              "  7.834249117877334e-05,\n",
              "  7.932948210509494e-05,\n",
              "  8.10938945505768e-05,\n",
              "  7.924786768853664e-05,\n",
              "  7.737479609204456e-05,\n",
              "  7.510372233809903e-05,\n",
              "  7.402255141641945e-05,\n",
              "  7.189184543676674e-05,\n",
              "  7.09086234564893e-05,\n",
              "  6.966428918531165e-05,\n",
              "  6.889102223794907e-05,\n",
              "  6.819338159402832e-05,\n",
              "  6.699581717839465e-05,\n",
              "  6.646394467679784e-05,\n",
              "  6.589908298337832e-05,\n",
              "  6.430060602724552e-05,\n",
              "  6.390376074705273e-05,\n",
              "  6.294929335126653e-05,\n",
              "  6.301603571046144e-05,\n",
              "  6.193420995259658e-05,\n",
              "  6.147291423985735e-05,\n",
              "  6.009358185110614e-05,\n",
              "  5.9414909628685564e-05,\n",
              "  5.914876237511635e-05,\n",
              "  5.8351681218482554e-05,\n",
              "  5.8007506595458835e-05,\n",
              "  5.711632547900081e-05,\n",
              "  5.625847916235216e-05,\n",
              "  5.572411464527249e-05,\n",
              "  5.572543159360066e-05,\n",
              "  5.581139703281224e-05,\n",
              "  5.440410677692853e-05,\n",
              "  5.3320043662097305e-05,\n",
              "  5.283202699501999e-05,\n",
              "  5.2224855608073995e-05,\n",
              "  5.1738566980930045e-05,\n",
              "  5.110739220981486e-05,\n",
              "  5.070740735391155e-05,\n",
              "  4.9760968977352604e-05,\n",
              "  4.9572558054933324e-05,\n",
              "  4.9183578084921464e-05,\n",
              "  4.826581061934121e-05,\n",
              "  4.8097856051754206e-05,\n",
              "  4.730663204099983e-05,\n",
              "  4.691477079177275e-05,\n",
              "  4.6336375817190856e-05,\n",
              "  4.586756040225737e-05,\n",
              "  4.55061745014973e-05,\n",
              "  4.496998371905647e-05,\n",
              "  4.439700205693953e-05,\n",
              "  4.401572005008347e-05,\n",
              "  4.346137211541645e-05,\n",
              "  4.31835105700884e-05,\n",
              "  4.302686284063384e-05,\n",
              "  4.2921015847241506e-05,\n",
              "  4.2606276110745966e-05,\n",
              "  4.1942086681956425e-05,\n",
              "  4.073714808328077e-05,\n",
              "  4.019806510768831e-05,\n",
              "  3.961804395657964e-05,\n",
              "  3.917253707186319e-05,\n",
              "  3.872217348543927e-05,\n",
              "  3.844585444312543e-05,\n",
              "  3.7808145862072706e-05,\n",
              "  3.756127989618108e-05,\n",
              "  3.706180723384023e-05,\n",
              "  3.7190540751907974e-05,\n",
              "  3.6789500882150605e-05,\n",
              "  3.6305602407082915e-05,\n",
              "  3.5486489650793374e-05,\n",
              "  3.520255631883629e-05,\n",
              "  3.509207090246491e-05,\n",
              "  3.4607717680046335e-05,\n",
              "  3.421789369895123e-05,\n",
              "  3.3597189030842856e-05,\n",
              "  3.347943857079372e-05,\n",
              "  3.2914122130023316e-05,\n",
              "  3.274977279943414e-05,\n",
              "  3.230691800126806e-05,\n",
              "  3.2251380616799e-05,\n",
              "  3.1796516850590706e-05,\n",
              "  3.162303983117454e-05,\n",
              "  3.1011215469334275e-05,\n",
              "  3.080961687373929e-05,\n",
              "  3.123661008430645e-05,\n",
              "  3.1002997275209054e-05,\n",
              "  3.065278724534437e-05,\n",
              "  3.016496339114383e-05,\n",
              "  2.9816401365678757e-05,\n",
              "  2.9143577194190584e-05,\n",
              "  2.888181188609451e-05,\n",
              "  2.8346119506750256e-05,\n",
              "  2.7993242838419974e-05,\n",
              "  2.7776299248216674e-05,\n",
              "  2.7431928174337372e-05,\n",
              "  2.707293060666416e-05,\n",
              "  2.6728690500021912e-05,\n",
              "  2.6587922548060305e-05,\n",
              "  2.6462081223144196e-05,\n",
              "  2.634386510180775e-05,\n",
              "  2.5701676349854097e-05,\n",
              "  2.5400386221008375e-05,\n",
              "  2.5081977582885884e-05,\n",
              "  2.5024424758157693e-05,\n",
              "  2.4752673198236153e-05,\n",
              "  2.443187804601621e-05,\n",
              "  2.4283403035951778e-05,\n",
              "  2.3946104192873463e-05,\n",
              "  2.3744260033708997e-05,\n",
              "  2.3620026695425622e-05,\n",
              "  2.3319978936342523e-05,\n",
              "  2.3125194275053218e-05,\n",
              "  2.29485103773186e-05,\n",
              "  2.257034066133201e-05,\n",
              "  2.246914846182335e-05,\n",
              "  2.2149850337882526e-05,\n",
              "  2.1958951037959196e-05,\n",
              "  2.1632275092997588e-05,\n",
              "  2.175228473788593e-05,\n",
              "  2.13502898986917e-05,\n",
              "  2.1037885744590312e-05,\n",
              "  2.078455509035848e-05,\n",
              "  2.0551848137984052e-05,\n",
              "  2.033177406701725e-05,\n",
              "  2.014418350881897e-05,\n",
              "  1.996016362681985e-05,\n",
              "  1.9662667909869924e-05,\n",
              "  1.9758248527068645e-05,\n",
              "  1.9491866623866372e-05,\n",
              "  1.9191682440578006e-05,\n",
              "  1.9028222595807165e-05,\n",
              "  1.874256668088492e-05,\n",
              "  1.8580782125354744e-05,\n",
              "  1.838126263464801e-05,\n",
              "  1.8206952518085018e-05,\n",
              "  1.8204387743026018e-05,\n",
              "  1.7874632249004208e-05,\n",
              "  1.7782947907107882e-05,\n",
              "  1.759998303896282e-05,\n",
              "  1.7463218682678416e-05,\n",
              "  1.7270789612666704e-05,\n",
              "  1.6990139556583017e-05,\n",
              "  1.6952042642515153e-05,\n",
              "  1.6765212421887554e-05,\n",
              "  1.6612777471891604e-05,\n",
              "  1.642665301915258e-05,\n",
              "  1.632746534596663e-05,\n",
              "  1.6203335690079257e-05,\n",
              "  1.5858080587349832e-05,\n",
              "  1.5738673027954064e-05,\n",
              "  1.5629846529918723e-05,\n",
              "  1.5488467397517525e-05,\n",
              "  1.5310204616980627e-05,\n",
              "  1.5248507224896457e-05,\n",
              "  1.5096952665771823e-05,\n",
              "  1.4871634448354598e-05,\n",
              "  1.4780881429032888e-05,\n",
              "  1.4638677384937182e-05,\n",
              "  1.4502292287943419e-05,\n",
              "  1.4455335076490883e-05,\n",
              "  1.4346183888847008e-05,\n",
              "  1.4177229786582757e-05,\n",
              "  1.3928466614743229e-05,\n",
              "  1.3894032235839404e-05,\n",
              "  1.3686285456060432e-05,\n",
              "  1.3530440810427535e-05,\n",
              "  1.3434184438665397e-05,\n",
              "  1.3293712072481867e-05,\n",
              "  1.3142578609404154e-05,\n",
              "  1.3160771231923718e-05,\n",
              "  1.2918523680127691e-05,\n",
              "  1.290929958486231e-05,\n",
              "  1.2702195817837492e-05,\n",
              "  1.2550112842291128e-05,\n",
              "  1.2472758498915937e-05,\n",
              "  1.232654267369071e-05,\n",
              "  1.2197416253911797e-05,\n",
              "  1.2077764949935954e-05,\n",
              "  1.1928912499570288e-05,\n",
              "  1.1780429304053541e-05,\n",
              "  1.177678677777294e-05,\n",
              "  1.1618525604717433e-05,\n",
              "  1.148951196228154e-05,\n",
              "  1.1363043995515909e-05,\n",
              "  1.143612007581396e-05,\n",
              "  1.1252089279878419e-05,\n",
              "  1.1090874068031553e-05,\n",
              "  1.0867580385820474e-05,\n",
              "  1.0806289537867997e-05,\n",
              "  1.0765838851511944e-05,\n",
              "  1.0595844287308864e-05,\n",
              "  1.0521245712880045e-05,\n",
              "  1.0359185580455232e-05,\n",
              "  1.0402156476629898e-05,\n",
              "  1.0294032108504325e-05,\n",
              "  1.0094423487316817e-05,\n",
              "  1.0044120244856458e-05,\n",
              "  1.0026183190348092e-05,\n",
              "  1.0135319826076739e-05,\n",
              "  9.98454743239563e-06,\n",
              "  9.853081792243756e-06,\n",
              "  9.619484444556292e-06,\n",
              "  9.492515346209984e-06,\n",
              "  9.332644367532339e-06,\n",
              "  9.203953595715575e-06,\n",
              "  9.137875167652965e-06,\n",
              "  8.999715646496043e-06,\n",
              "  8.957552381616551e-06,\n",
              "  8.81516098161228e-06,\n",
              "  8.75924979482079e-06,\n",
              "  8.676318429934327e-06,\n",
              "  8.647909453429747e-06,\n",
              "  8.549895937903784e-06,\n",
              "  8.425856321991887e-06,\n",
              "  8.3742052083835e-06,\n",
              "  8.274328138213605e-06,\n",
              "  8.202998287742957e-06,\n",
              "  8.132249604386743e-06,\n",
              "  8.075598998402711e-06,\n",
              "  7.957073648867663e-06,\n",
              "  7.920975804154295e-06,\n",
              "  7.927877049951348e-06,\n",
              "  7.824054591765162e-06,\n",
              "  7.733317943348084e-06,\n",
              "  7.502109383494826e-06,\n",
              "  7.599198852403788e-06,\n",
              "  7.506378096877597e-06,\n",
              "  7.472368906746851e-06,\n",
              "  7.3761279963946436e-06,\n",
              "  7.296859621419571e-06,\n",
              "  7.186911261669593e-06,\n",
              "  7.098001333361026e-06,\n",
              "  7.077157533785794e-06,\n",
              "  6.964752174098976e-06,\n",
              "  6.916616712260293e-06,\n",
              "  6.834960004198365e-06,\n",
              "  6.819984719186323e-06,\n",
              "  6.805504654039396e-06,\n",
              "  6.688153007416986e-06,\n",
              "  6.6307384258834645e-06,\n",
              "  6.549938007083256e-06,\n",
              "  6.481478067144053e-06,\n",
              "  6.434118859033333e-06,\n",
              "  6.354324796120636e-06,\n",
              "  6.293820661085192e-06,\n",
              "  6.252181265153922e-06,\n",
              "  6.186580321809743e-06,\n",
              "  6.309531272563618e-06,\n",
              "  6.1867726799391676e-06,\n",
              "  6.207202204677742e-06,\n",
              "  6.114210464147618e-06,\n",
              "  5.9923186199739575e-06,\n",
              "  5.945866178080905e-06,\n",
              "  5.8296104725741316e-06,\n",
              "  5.735117611038731e-06,\n",
              "  5.717558451578952e-06,\n",
              "  5.621239779429743e-06,\n",
              "  5.579984190262621e-06,\n",
              "  5.538512596103828e-06,\n",
              "  5.438989319372922e-06,\n",
              "  5.410969151853351e-06,\n",
              "  5.4201668717723805e-06,\n",
              "  5.354499080567621e-06,\n",
              "  5.2868458624288905e-06,\n",
              "  5.230049737292575e-06,\n",
              "  5.1971910579595715e-06,\n",
              "  5.143190719536506e-06,\n",
              "  5.075688932265621e-06,\n",
              "  5.025192422181135e-06,\n",
              "  5.009809228795348e-06,\n",
              "  4.929766419081716e-06,\n",
              "  4.911356882075779e-06,\n",
              "  4.922629159409553e-06,\n",
              "  4.8440738282806706e-06,\n",
              "  4.782101314049214e-06,\n",
              "  4.71657995149144e-06,\n",
              "  4.678207460528938e-06,\n",
              "  4.642696694645565e-06,\n",
              "  4.59501188743161e-06,\n",
              "  4.561960395221831e-06,\n",
              "  4.504931439441862e-06,\n",
              "  4.466665359359467e-06,\n",
              "  4.419097422214691e-06,\n",
              "  4.394356437842362e-06,\n",
              "  4.353544227342354e-06,\n",
              "  4.332638582127402e-06,\n",
              "  4.2716419557109475e-06,\n",
              "  4.246748176228721e-06],\n",
              " 'val_loss': [0.5128041505813599,\n",
              "  0.3805030584335327,\n",
              "  0.2900426387786865,\n",
              "  0.22875992953777313,\n",
              "  0.19009622931480408,\n",
              "  0.16635790467262268,\n",
              "  0.1476704478263855,\n",
              "  0.13417300581932068,\n",
              "  0.1258605718612671,\n",
              "  0.11706343293190002,\n",
              "  0.11188038438558578,\n",
              "  0.10995027422904968,\n",
              "  0.10674050450325012,\n",
              "  0.10454149544239044,\n",
              "  0.1023988127708435,\n",
              "  0.1012384295463562,\n",
              "  0.10152413696050644,\n",
              "  0.09963592141866684,\n",
              "  0.09821886569261551,\n",
              "  0.10183718055486679,\n",
              "  0.09941539168357849,\n",
              "  0.09903190284967422,\n",
              "  0.09843254089355469,\n",
              "  0.09610814601182938,\n",
              "  0.09891831874847412,\n",
              "  0.09339504688978195,\n",
              "  0.09345754981040955,\n",
              "  0.09170812368392944,\n",
              "  0.0934678390622139,\n",
              "  0.09488555788993835,\n",
              "  0.09357583522796631,\n",
              "  0.09530151635408401,\n",
              "  0.09503749012947083,\n",
              "  0.09399355202913284,\n",
              "  0.09595971554517746,\n",
              "  0.09419041872024536,\n",
              "  0.09430070221424103,\n",
              "  0.0935758426785469,\n",
              "  0.09385818243026733,\n",
              "  0.09524834156036377,\n",
              "  0.09701750427484512,\n",
              "  0.09766557812690735,\n",
              "  0.09776013344526291,\n",
              "  0.09836018830537796,\n",
              "  0.09668352454900742,\n",
              "  0.09674753993749619,\n",
              "  0.09786495566368103,\n",
              "  0.09700269997119904,\n",
              "  0.0999206081032753,\n",
              "  0.10034756362438202,\n",
              "  0.09839484095573425,\n",
              "  0.09949781000614166,\n",
              "  0.10004879534244537,\n",
              "  0.10226583480834961,\n",
              "  0.10186851769685745,\n",
              "  0.10261847078800201,\n",
              "  0.0979321151971817,\n",
              "  0.09951509535312653,\n",
              "  0.10285256803035736,\n",
              "  0.10367486625909805,\n",
              "  0.10249518603086472,\n",
              "  0.10217330604791641,\n",
              "  0.10517271608114243,\n",
              "  0.1061282679438591,\n",
              "  0.10906694084405899,\n",
              "  0.10526569187641144,\n",
              "  0.1069253459572792,\n",
              "  0.10677620768547058,\n",
              "  0.10653405636548996,\n",
              "  0.10717635601758957,\n",
              "  0.1079513281583786,\n",
              "  0.10853981226682663,\n",
              "  0.11041632294654846,\n",
              "  0.10902468860149384,\n",
              "  0.11014631390571594,\n",
              "  0.10947413742542267,\n",
              "  0.1143646314740181,\n",
              "  0.11337102204561234,\n",
              "  0.11176571249961853,\n",
              "  0.11066029965877533,\n",
              "  0.11127051711082458,\n",
              "  0.11265168339014053,\n",
              "  0.11318670958280563,\n",
              "  0.11251584440469742,\n",
              "  0.11518952250480652,\n",
              "  0.11455494910478592,\n",
              "  0.11554354429244995,\n",
              "  0.11305082589387894,\n",
              "  0.11563034355640411,\n",
              "  0.11880453675985336,\n",
              "  0.11729104816913605,\n",
              "  0.11768288165330887,\n",
              "  0.11718976497650146,\n",
              "  0.12138865888118744,\n",
              "  0.12002366781234741,\n",
              "  0.11825325340032578,\n",
              "  0.12095203250646591,\n",
              "  0.12208463996648788,\n",
              "  0.12298724800348282,\n",
              "  0.1211094781756401,\n",
              "  0.12237152457237244,\n",
              "  0.12256131321191788,\n",
              "  0.12398818135261536,\n",
              "  0.12441576272249222,\n",
              "  0.12529213726520538,\n",
              "  0.12557271122932434,\n",
              "  0.12761065363883972,\n",
              "  0.12844079732894897,\n",
              "  0.1184634193778038,\n",
              "  0.12103275954723358,\n",
              "  0.12451354414224625,\n",
              "  0.12822593748569489,\n",
              "  0.1276625543832779,\n",
              "  0.12856677174568176,\n",
              "  0.1290491670370102,\n",
              "  0.1276058852672577,\n",
              "  0.1299881786108017,\n",
              "  0.1347258985042572,\n",
              "  0.1313927173614502,\n",
              "  0.13198000192642212,\n",
              "  0.1342911571264267,\n",
              "  0.1351667046546936,\n",
              "  0.13438917696475983,\n",
              "  0.134319469332695,\n",
              "  0.13503322005271912,\n",
              "  0.13622619211673737,\n",
              "  0.13444532454013824,\n",
              "  0.13687771558761597,\n",
              "  0.1369491070508957,\n",
              "  0.1372482180595398,\n",
              "  0.13091449439525604,\n",
              "  0.13257186114788055,\n",
              "  0.13498686254024506,\n",
              "  0.13734331727027893,\n",
              "  0.13810773193836212,\n",
              "  0.13903294503688812,\n",
              "  0.13983437418937683,\n",
              "  0.14040303230285645,\n",
              "  0.14142464101314545,\n",
              "  0.13558968901634216,\n",
              "  0.13918043673038483,\n",
              "  0.14299817383289337,\n",
              "  0.14312420785427094,\n",
              "  0.14232754707336426,\n",
              "  0.14365115761756897,\n",
              "  0.14486952126026154,\n",
              "  0.14477311074733734,\n",
              "  0.14517316222190857,\n",
              "  0.14629025757312775,\n",
              "  0.14644551277160645,\n",
              "  0.14036904275417328,\n",
              "  0.14194157719612122,\n",
              "  0.1439070999622345,\n",
              "  0.14559124410152435,\n",
              "  0.14691190421581268,\n",
              "  0.144659623503685,\n",
              "  0.14506348967552185,\n",
              "  0.1479419618844986,\n",
              "  0.14862141013145447,\n",
              "  0.150183767080307,\n",
              "  0.15154671669006348,\n",
              "  0.152256041765213,\n",
              "  0.15446725487709045,\n",
              "  0.15462981164455414,\n",
              "  0.15481393039226532,\n",
              "  0.15479235351085663,\n",
              "  0.15498483180999756,\n",
              "  0.155532568693161,\n",
              "  0.15703198313713074,\n",
              "  0.15768100321292877,\n",
              "  0.15821529924869537,\n",
              "  0.15917591750621796,\n",
              "  0.15911203622817993,\n",
              "  0.15981827676296234,\n",
              "  0.1587039977312088,\n",
              "  0.1592031568288803,\n",
              "  0.16175390779972076,\n",
              "  0.16074684262275696,\n",
              "  0.16167491674423218,\n",
              "  0.16222412884235382,\n",
              "  0.1619240641593933,\n",
              "  0.16251634061336517,\n",
              "  0.16317340731620789,\n",
              "  0.16315865516662598,\n",
              "  0.16324883699417114,\n",
              "  0.16089317202568054,\n",
              "  0.16254699230194092,\n",
              "  0.16473466157913208,\n",
              "  0.1654890924692154,\n",
              "  0.16647325456142426,\n",
              "  0.16734203696250916,\n",
              "  0.16720910370349884,\n",
              "  0.16802483797073364,\n",
              "  0.16720004379749298,\n",
              "  0.16782160103321075,\n",
              "  0.16859155893325806,\n",
              "  0.1686585396528244,\n",
              "  0.16894574463367462,\n",
              "  0.17119039595127106,\n",
              "  0.16763879358768463,\n",
              "  0.16869670152664185,\n",
              "  0.17057020962238312,\n",
              "  0.17231489717960358,\n",
              "  0.1720973253250122,\n",
              "  0.1723511964082718,\n",
              "  0.17314286530017853,\n",
              "  0.17477738857269287,\n",
              "  0.17448176443576813,\n",
              "  0.1735762655735016,\n",
              "  0.1745808869600296,\n",
              "  0.17466503381729126,\n",
              "  0.17537075281143188,\n",
              "  0.17632484436035156,\n",
              "  0.1759224385023117,\n",
              "  0.17508402466773987,\n",
              "  0.17514975368976593,\n",
              "  0.17621447145938873,\n",
              "  0.17694076895713806,\n",
              "  0.17772093415260315,\n",
              "  0.17588168382644653,\n",
              "  0.17339900135993958,\n",
              "  0.17423203587532043,\n",
              "  0.1775452196598053,\n",
              "  0.17776702344417572,\n",
              "  0.1782480925321579,\n",
              "  0.17931996285915375,\n",
              "  0.17975997924804688,\n",
              "  0.17965741455554962,\n",
              "  0.17973247170448303,\n",
              "  0.17991933226585388,\n",
              "  0.181246817111969,\n",
              "  0.18141770362854004,\n",
              "  0.18145789206027985,\n",
              "  0.18199357390403748,\n",
              "  0.18391107022762299,\n",
              "  0.18420696258544922,\n",
              "  0.18414460122585297,\n",
              "  0.18406018614768982,\n",
              "  0.1845206916332245,\n",
              "  0.1869509071111679,\n",
              "  0.1869625598192215,\n",
              "  0.18607835471630096,\n",
              "  0.18679441511631012,\n",
              "  0.18696455657482147,\n",
              "  0.18396008014678955,\n",
              "  0.1848638504743576,\n",
              "  0.18547271192073822,\n",
              "  0.18697576224803925,\n",
              "  0.1875445693731308,\n",
              "  0.1881166249513626,\n",
              "  0.18894945085048676,\n",
              "  0.1893714964389801,\n",
              "  0.18942983448505402,\n",
              "  0.19025762379169464,\n",
              "  0.19046562910079956,\n",
              "  0.1902293711900711,\n",
              "  0.19107574224472046,\n",
              "  0.19159933924674988,\n",
              "  0.1916671246290207,\n",
              "  0.19218118488788605,\n",
              "  0.19283252954483032,\n",
              "  0.1930578500032425,\n",
              "  0.1930750459432602,\n",
              "  0.19354012608528137,\n",
              "  0.19561699032783508,\n",
              "  0.19606825709342957,\n",
              "  0.19539964199066162,\n",
              "  0.19466444849967957,\n",
              "  0.19552546739578247,\n",
              "  0.1927562803030014,\n",
              "  0.19312764704227448,\n",
              "  0.19381661713123322,\n",
              "  0.19423192739486694,\n",
              "  0.19498124718666077,\n",
              "  0.1954425722360611,\n",
              "  0.1965867429971695,\n",
              "  0.19692163169384003,\n",
              "  0.19740240275859833,\n",
              "  0.19776847958564758,\n",
              "  0.19810880720615387,\n",
              "  0.1982434242963791,\n",
              "  0.19833989441394806,\n",
              "  0.19943492114543915,\n",
              "  0.19922716915607452,\n",
              "  0.1998155564069748,\n",
              "  0.1999925822019577,\n",
              "  0.20031599700450897,\n",
              "  0.1977514922618866,\n",
              "  0.19833382964134216,\n",
              "  0.19879230856895447,\n",
              "  0.1998620331287384,\n",
              "  0.20098371803760529,\n",
              "  0.2014700472354889,\n",
              "  0.20145626366138458,\n",
              "  0.20438352227210999,\n",
              "  0.2042647749185562,\n",
              "  0.2033078372478485,\n",
              "  0.20538222789764404,\n",
              "  0.20425532758235931,\n",
              "  0.2048504650592804,\n",
              "  0.20494645833969116,\n",
              "  0.205355703830719,\n",
              "  0.20540829002857208,\n",
              "  0.20585478842258453,\n",
              "  0.20654556155204773,\n",
              "  0.20619238913059235,\n",
              "  0.20734795928001404,\n",
              "  0.20739498734474182,\n",
              "  0.2082151621580124,\n",
              "  0.2078668177127838,\n",
              "  0.20853781700134277,\n",
              "  0.20937786996364594,\n",
              "  0.20963215827941895,\n",
              "  0.20658032596111298,\n",
              "  0.20770993828773499,\n",
              "  0.20904722809791565,\n",
              "  0.20985642075538635,\n",
              "  0.21070849895477295,\n",
              "  0.2114865928888321,\n",
              "  0.21164575219154358,\n",
              "  0.21166032552719116,\n",
              "  0.21198202669620514,\n",
              "  0.21087603271007538,\n",
              "  0.21260887384414673,\n",
              "  0.21347849071025848,\n",
              "  0.21367785334587097,\n",
              "  0.21450066566467285,\n",
              "  0.21437209844589233,\n",
              "  0.21472889184951782,\n",
              "  0.21535587310791016,\n",
              "  0.2160487025976181,\n",
              "  0.21494562923908234,\n",
              "  0.21599023044109344,\n",
              "  0.21612165868282318,\n",
              "  0.21617743372917175,\n",
              "  0.21673423051834106,\n",
              "  0.2170291692018509,\n",
              "  0.21731293201446533,\n",
              "  0.21720093488693237,\n",
              "  0.2172713577747345,\n",
              "  0.21792511641979218,\n",
              "  0.21861907839775085,\n",
              "  0.21890580654144287,\n",
              "  0.21874159574508667,\n",
              "  0.22009259462356567,\n",
              "  0.2200961410999298,\n",
              "  0.22011464834213257,\n",
              "  0.22035323083400726,\n",
              "  0.22043286263942719,\n",
              "  0.2210293412208557,\n",
              "  0.22121544182300568,\n",
              "  0.22083334624767303,\n",
              "  0.22051332890987396,\n",
              "  0.2218465358018875,\n",
              "  0.22174997627735138,\n",
              "  0.22228391468524933,\n",
              "  0.2224808633327484,\n",
              "  0.22358009219169617,\n",
              "  0.2234748750925064,\n",
              "  0.22349758446216583,\n",
              "  0.22344417870044708,\n",
              "  0.22517435252666473,\n",
              "  0.22534099221229553,\n",
              "  0.22527971863746643,\n",
              "  0.22586306929588318,\n",
              "  0.2262372076511383,\n",
              "  0.2253383845090866,\n",
              "  0.22625768184661865,\n",
              "  0.22713838517665863,\n",
              "  0.2273261547088623,\n",
              "  0.22825801372528076,\n",
              "  0.2292194962501526,\n",
              "  0.22896897792816162,\n",
              "  0.2289951741695404,\n",
              "  0.22851331532001495,\n",
              "  0.22925563156604767,\n",
              "  0.22994458675384521,\n",
              "  0.2294260561466217,\n",
              "  0.22846384346485138,\n",
              "  0.22928526997566223,\n",
              "  0.22970256209373474,\n",
              "  0.23091810941696167,\n",
              "  0.23157523572444916,\n",
              "  0.23111699521541595,\n",
              "  0.23166467249393463,\n",
              "  0.23176610469818115,\n",
              "  0.23181748390197754,\n",
              "  0.23258861899375916,\n",
              "  0.23227576911449432,\n",
              "  0.2324826717376709,\n",
              "  0.23329758644104004,\n",
              "  0.233852356672287,\n",
              "  0.23309476673603058,\n",
              "  0.2330198734998703,\n",
              "  0.23449161648750305,\n",
              "  0.23510001599788666,\n",
              "  0.23417222499847412,\n",
              "  0.23446455597877502,\n",
              "  0.23468303680419922,\n",
              "  0.23525382578372955,\n",
              "  0.23589082062244415,\n",
              "  0.2356559783220291,\n",
              "  0.23714157938957214,\n",
              "  0.2369416207075119,\n",
              "  0.2367023527622223,\n",
              "  0.23731845617294312,\n",
              "  0.23699088394641876,\n",
              "  0.23746077716350555,\n",
              "  0.23793384432792664,\n",
              "  0.2383703887462616,\n",
              "  0.23885118961334229,\n",
              "  0.23827168345451355,\n",
              "  0.23916229605674744,\n",
              "  0.23944777250289917,\n",
              "  0.24045948684215546,\n",
              "  0.24012315273284912,\n",
              "  0.2405562698841095,\n",
              "  0.24069327116012573,\n",
              "  0.2408401072025299,\n",
              "  0.24155102670192719,\n",
              "  0.24155692756175995,\n",
              "  0.24186567962169647,\n",
              "  0.2422499805688858,\n",
              "  0.24254541099071503,\n",
              "  0.24372054636478424,\n",
              "  0.2442564070224762,\n",
              "  0.24356752634048462,\n",
              "  0.24384579062461853,\n",
              "  0.24409063160419464,\n",
              "  0.24305570125579834,\n",
              "  0.24351252615451813,\n",
              "  0.2450585812330246,\n",
              "  0.2449490875005722,\n",
              "  0.2445646971464157,\n",
              "  0.24527350068092346,\n",
              "  0.2466541975736618,\n",
              "  0.24731135368347168,\n",
              "  0.2475770115852356,\n",
              "  0.24810509383678436,\n",
              "  0.24860259890556335,\n",
              "  0.24793021380901337,\n",
              "  0.24833397567272186,\n",
              "  0.24912165105342865,\n",
              "  0.24961231648921967,\n",
              "  0.24941447377204895,\n",
              "  0.24933189153671265,\n",
              "  0.2502222955226898,\n",
              "  0.2501978874206543,\n",
              "  0.24984684586524963,\n",
              "  0.24929657578468323,\n",
              "  0.2504357099533081,\n",
              "  0.2502317726612091,\n",
              "  0.2503848969936371,\n",
              "  0.25133588910102844,\n",
              "  0.2516571879386902,\n",
              "  0.251878559589386,\n",
              "  0.2515008747577667,\n",
              "  0.2518810033798218,\n",
              "  0.25132015347480774,\n",
              "  0.2525496482849121,\n",
              "  0.25307920575141907,\n",
              "  0.25347596406936646,\n",
              "  0.25359103083610535,\n",
              "  0.25407537817955017,\n",
              "  0.25423571467399597,\n",
              "  0.253841370344162,\n",
              "  0.2540341019630432,\n",
              "  0.2549628019332886,\n",
              "  0.2547014653682709,\n",
              "  0.25544217228889465,\n",
              "  0.25568845868110657,\n",
              "  0.2560822665691376,\n",
              "  0.2562539577484131,\n",
              "  0.25688523054122925,\n",
              "  0.25694265961647034,\n",
              "  0.2567390203475952,\n",
              "  0.25729498267173767,\n",
              "  0.2572104036808014,\n",
              "  0.2577260434627533,\n",
              "  0.2584264278411865,\n",
              "  0.25791963934898376,\n",
              "  0.2588006258010864,\n",
              "  0.2591778337955475,\n",
              "  0.25571301579475403,\n",
              "  0.25611817836761475,\n",
              "  0.25684720277786255,\n",
              "  0.2578047811985016,\n",
              "  0.2592109143733978,\n",
              "  0.2601824998855591,\n",
              "  0.2600349485874176,\n",
              "  0.26104000210762024,\n",
              "  0.2613937258720398,\n",
              "  0.26170527935028076,\n",
              "  0.2618519961833954,\n",
              "  0.26193127036094666,\n",
              "  0.2617020308971405,\n",
              "  0.2620222568511963,\n",
              "  0.26293841004371643,\n",
              "  0.2629285752773285,\n",
              "  0.26340046525001526,\n",
              "  0.2637578547000885,\n",
              "  0.26361897587776184,\n",
              "  0.2643511891365051,\n",
              "  0.2643122971057892,\n",
              "  0.2644951641559601,\n",
              "  0.26212450861930847,\n",
              "  0.26256418228149414,\n",
              "  0.2633771598339081,\n",
              "  0.2641353905200958,\n",
              "  0.2632400691509247,\n",
              "  0.26391398906707764,\n",
              "  0.26446664333343506,\n",
              "  0.2665846645832062,\n",
              "  0.26667487621307373,\n",
              "  0.2670060396194458,\n",
              "  0.26712632179260254,\n",
              "  0.26748088002204895,\n",
              "  0.2677942216396332,\n",
              "  0.2680686414241791,\n",
              "  0.2683449983596802,\n",
              "  0.2685196101665497,\n",
              "  0.26852425932884216,\n",
              "  0.268811970949173,\n",
              "  0.269686222076416,\n",
              "  0.269668310880661,\n",
              "  0.26956188678741455,\n",
              "  0.2690698504447937,\n",
              "  0.2697332799434662,\n",
              "  0.27046069502830505,\n",
              "  0.27074968814849854,\n",
              "  0.27082377672195435,\n",
              "  0.2711339592933655,\n",
              "  0.2715930640697479,\n",
              "  0.2725560963153839,\n",
              "  0.272346556186676,\n",
              "  0.2725145220756531,\n",
              "  0.27093997597694397,\n",
              "  0.2711631953716278,\n",
              "  0.2727571427822113,\n",
              "  0.2738528549671173,\n",
              "  0.27428877353668213,\n",
              "  0.27441075444221497,\n",
              "  0.2744804918766022,\n",
              "  0.2756285071372986,\n",
              "  0.27466776967048645,\n",
              "  0.2751280665397644,\n",
              "  0.27597856521606445,\n",
              "  0.27592021226882935,\n",
              "  0.2771737575531006,\n",
              "  0.27640241384506226,\n",
              "  0.27681097388267517,\n",
              "  0.27710795402526855,\n",
              "  0.27721816301345825,\n",
              "  0.27733102440834045,\n",
              "  0.27686014771461487,\n",
              "  0.27765241265296936,\n",
              "  0.27799543738365173,\n",
              "  0.27808821201324463,\n",
              "  0.2781902551651001,\n",
              "  0.27719148993492126,\n",
              "  0.27873045206069946,\n",
              "  0.27951061725616455,\n",
              "  0.28005850315093994,\n",
              "  0.2800886034965515,\n",
              "  0.2796994149684906,\n",
              "  0.28033992648124695,\n",
              "  0.2808622419834137,\n",
              "  0.280801385641098,\n",
              "  0.28146031498908997,\n",
              "  0.2810984253883362,\n",
              "  0.28131401538848877,\n",
              "  0.28210246562957764,\n",
              "  0.2821868658065796,\n",
              "  0.28060561418533325,\n",
              "  0.28119614720344543,\n",
              "  0.2823537588119507,\n",
              "  0.2829494774341583,\n",
              "  0.28243130445480347,\n",
              "  0.2830789387226105,\n",
              "  0.2838118374347687,\n",
              "  0.28353533148765564,\n",
              "  0.28382036089897156,\n",
              "  0.284553200006485,\n",
              "  0.28600916266441345,\n",
              "  0.28639519214630127,\n",
              "  0.285733163356781,\n",
              "  0.2850249409675598,\n",
              "  0.2856791317462921,\n",
              "  0.28586360812187195,\n",
              "  0.2867391109466553,\n",
              "  0.2866433560848236,\n",
              "  0.28688937425613403,\n",
              "  0.2868707478046417,\n",
              "  0.28707632422447205,\n",
              "  0.28792402148246765,\n",
              "  0.28750768303871155,\n",
              "  0.2887111008167267,\n",
              "  0.2893226444721222,\n",
              "  0.2892903983592987,\n",
              "  0.28924456238746643]}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "J0Bm7CI7Uxte",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "0d7e9a99-aa57-4dc8-f40e-5fd7a571696e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f49da543a30>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "lossdf = pd.DataFrame(ann.history.history)\n",
        "lossdf.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "IcDmQALuXIOo"
      },
      "outputs": [],
      "source": [
        "#low training error -> low bias\n",
        "#high test error    -> high variance\n",
        "#Overfitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "9ExdIYn7X6ZH"
      },
      "outputs": [],
      "source": [
        "ann = Sequential()\n",
        "ann.add(Dense(units=30, activation=\"relu\"))\n",
        "ann.add(Dense(units=15, activation=\"relu\"))\n",
        "ann.add(Dense(units=1, activation=\"sigmoid\"))\n",
        "ann.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Ovo1TSJvYqdJ"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "the following techniques to prevent Overfitting in neural networks:\n",
        "\n",
        "Dropout\n",
        "Early Stopping\n",
        "Weight Decay\n",
        "'''\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "UzcGITydYx7I"
      },
      "outputs": [],
      "source": [
        "# this is like teeling.. as u get the min validation ,  inform me..\n",
        "earlystop = EarlyStopping(monitor=\"val_loss\", mode=\"min\", verbose=1, patience=25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "_UbnSfPFZFct",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5910b750-8d6b-41a0-ede3-50552f4b32e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/600\n",
            "15/15 [==============================] - 1s 13ms/step - loss: 0.7885 - val_loss: 0.6309\n",
            "Epoch 2/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.5139 - val_loss: 0.4761\n",
            "Epoch 3/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.3678 - val_loss: 0.3836\n",
            "Epoch 4/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.2878 - val_loss: 0.3207\n",
            "Epoch 5/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.2320 - val_loss: 0.2769\n",
            "Epoch 6/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.1905 - val_loss: 0.2400\n",
            "Epoch 7/600\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 0.1578 - val_loss: 0.2092\n",
            "Epoch 8/600\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1318 - val_loss: 0.1843\n",
            "Epoch 9/600\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.1121 - val_loss: 0.1631\n",
            "Epoch 10/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0970 - val_loss: 0.1468\n",
            "Epoch 11/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0866 - val_loss: 0.1357\n",
            "Epoch 12/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0782 - val_loss: 0.1248\n",
            "Epoch 13/600\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.0717 - val_loss: 0.1177\n",
            "Epoch 14/600\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.0663 - val_loss: 0.1145\n",
            "Epoch 15/600\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.0621 - val_loss: 0.1116\n",
            "Epoch 16/600\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.0586 - val_loss: 0.1085\n",
            "Epoch 17/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0554 - val_loss: 0.1055\n",
            "Epoch 18/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0523 - val_loss: 0.1037\n",
            "Epoch 19/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0497 - val_loss: 0.0998\n",
            "Epoch 20/600\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.0473 - val_loss: 0.1011\n",
            "Epoch 21/600\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.0449 - val_loss: 0.1001\n",
            "Epoch 22/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0430 - val_loss: 0.0979\n",
            "Epoch 23/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0410 - val_loss: 0.0972\n",
            "Epoch 24/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0393 - val_loss: 0.0972\n",
            "Epoch 25/600\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.0382 - val_loss: 0.0949\n",
            "Epoch 26/600\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.0365 - val_loss: 0.0977\n",
            "Epoch 27/600\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 0.0354 - val_loss: 0.0932\n",
            "Epoch 28/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0333 - val_loss: 0.0944\n",
            "Epoch 29/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0321 - val_loss: 0.0958\n",
            "Epoch 30/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0308 - val_loss: 0.0955\n",
            "Epoch 31/600\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.0295 - val_loss: 0.0953\n",
            "Epoch 32/600\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.0282 - val_loss: 0.0974\n",
            "Epoch 33/600\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.0276 - val_loss: 0.0957\n",
            "Epoch 34/600\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.0262 - val_loss: 0.0959\n",
            "Epoch 35/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0252 - val_loss: 0.0953\n",
            "Epoch 36/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0246 - val_loss: 0.1005\n",
            "Epoch 37/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0235 - val_loss: 0.0996\n",
            "Epoch 38/600\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.0225 - val_loss: 0.0981\n",
            "Epoch 39/600\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.0218 - val_loss: 0.0993\n",
            "Epoch 40/600\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.0210 - val_loss: 0.1014\n",
            "Epoch 41/600\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.0202 - val_loss: 0.1007\n",
            "Epoch 42/600\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.0197 - val_loss: 0.0930\n",
            "Epoch 43/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0191 - val_loss: 0.0988\n",
            "Epoch 44/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0179 - val_loss: 0.1086\n",
            "Epoch 45/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0171 - val_loss: 0.1162\n",
            "Epoch 46/600\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.0169 - val_loss: 0.1184\n",
            "Epoch 47/600\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 0.0171 - val_loss: 0.1081\n",
            "Epoch 48/600\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 0.0156 - val_loss: 0.1193\n",
            "Epoch 49/600\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 0.0149 - val_loss: 0.1224\n",
            "Epoch 50/600\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.0141 - val_loss: 0.1217\n",
            "Epoch 51/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0135 - val_loss: 0.1230\n",
            "Epoch 52/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0130 - val_loss: 0.1206\n",
            "Epoch 53/600\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 0.0126 - val_loss: 0.1244\n",
            "Epoch 54/600\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.0121 - val_loss: 0.1304\n",
            "Epoch 55/600\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.0116 - val_loss: 0.1300\n",
            "Epoch 56/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0112 - val_loss: 0.1360\n",
            "Epoch 57/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0111 - val_loss: 0.1351\n",
            "Epoch 58/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0104 - val_loss: 0.1298\n",
            "Epoch 59/600\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 0.0099 - val_loss: 0.1395\n",
            "Epoch 60/600\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.0095 - val_loss: 0.1429\n",
            "Epoch 61/600\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.0090 - val_loss: 0.1457\n",
            "Epoch 62/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0087 - val_loss: 0.1468\n",
            "Epoch 63/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0083 - val_loss: 0.1471\n",
            "Epoch 64/600\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.0079 - val_loss: 0.1446\n",
            "Epoch 65/600\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.0079 - val_loss: 0.1434\n",
            "Epoch 66/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0075 - val_loss: 0.1481\n",
            "Epoch 67/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0072 - val_loss: 0.1555\n",
            "Epoch 67: early stopping\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f49da45be20>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "ann.fit(xtrain, ytrain, epochs=600, validation_data=(xtest, ytest), callbacks=[earlystop])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "KLVz72TLZVRv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "194b99a0-deb6-4d5d-c66d-651a7edbdcda"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f49da392850>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXhb5Zn38e+txfsax4kT24mdkIUQQwAnBcpSKHtbMi0tSymFTgvvtCzdhil0YRgGZjplhrYzpe0wDKX0ooWUtjQtaQMDoRDKEgcCZCELWW2SeIljx4ssWbrfPx45URwnVhzZsuT7c126pHN0pHPbln/n0XOec46oKsYYY1KfJ9kFGGOMSQwLdGOMSRMW6MYYkyYs0I0xJk1YoBtjTJrwJWvF48eP16qqqmSt3hhjUtLKlSubVbV0oOeSFuhVVVXU1dUla/XGGJOSRGTb4Z6zLhdjjEkTFujGGJMm4gp0EblYRNaLyCYRuX2A56eIyDIReVNE3haRSxNfqjHGmCMZtA9dRLzAA8AFQD2wQkQWq+ramMW+DSxS1Z+IyBxgCVA1DPUaY1JcKBSivr6eQCCQ7FJGtaysLCoqKvD7/XG/Jp6doguATaq6GUBEHgcWArGBrkBB9HEh8H7cFRhjxpT6+nry8/OpqqpCRJJdzqikqrS0tFBfX091dXXcr4uny6Uc2BEzXR+dF+su4DMiUo9rnd8y0BuJyI0iUicidU1NTXEXaYxJH4FAgJKSEgvzIxARSkpKjvpbTKJ2il4NPKKqFcClwC9E5JD3VtUHVbVWVWtLSwccRmmMGQMszAc3lN9RPIHeAFTGTFdE58X6PLAIQFVfAbKA8UddTRxWbN3D9/78LpGInfbXGGNixRPoK4AZIlItIhnAVcDifstsBz4MICLH4wJ9WPpU3tqxlx+/8B77enqH4+2NMWNAXl5esksYFoMGuqr2AjcDS4F1uNEsa0TkbhG5LLrY14EbROQt4FfA9TpMV84oyHZ7fNu7Q8Px9sYYk7Li6kNX1SWqOlNVp6vqvdF5d6rq4ujjtar6QVU9SVXnqeozw1VwYTTQ2yzQjTHHSFW57bbbmDt3LjU1NTzxxBMA7Ny5k7PPPpt58+Yxd+5cXnrpJcLhMNdff/3+Zb///e8nufpDJe1cLkNlgW5M+vinP6xh7fvtCX3POZML+MePnRDXsr/97W9ZtWoVb731Fs3NzcyfP5+zzz6bX/7yl1x00UV861vfIhwO09XVxapVq2hoaGD16tUA7N27N6F1J0LKHfpvgW6MSZTly5dz9dVX4/V6mThxIueccw4rVqxg/vz5/OxnP+Ouu+7inXfeIT8/n2nTprF582ZuueUW/vznP1NQUDD4CkaYtdCNMUkTb0t6pJ199tm8+OKLPP3001x//fV87Wtf47Of/SxvvfUWS5cu5ac//SmLFi3i4YcfTnapB7EWujFmzDrrrLN44oknCIfDNDU18eKLL7JgwQK2bdvGxIkTueGGG/jCF77AG2+8QXNzM5FIhMsvv5x77rmHN954I9nlHyLlWug5GV58HrFAN8Ycs49//OO88sornHTSSYgI3/ve9ygrK+PnP/859913H36/n7y8PB599FEaGhr43Oc+RyQSAeBf//Vfk1z9oVIu0EWEwmy/BboxZsg6OjoAlyf33Xcf991330HPX3fddVx33XWHvG40tspjpVyXC2CBbowxA0jJQC/I9tuBRcYY009KBrq10I0x5lAW6MYYkyYs0I0xJk2kbKC3d4fsFLrGGBMjZQM9otARtFPoGmNMn5QNdIC2Lut2McYMryOdO33r1q3MnTt3BKs5spQM9AI7/N8YYw6RckeKwoEWuo1FNybF/el22PVOYt+zrAYu+e5hn7799tuprKzkpptuAuCuu+7C5/OxbNkyWltbCYVC3HPPPSxcuPCoVhsIBPjiF79IXV0dPp+P+++/n3PPPZc1a9bwuc99jmAwSCQS4Te/+Q2TJ0/miiuuoL6+nnA4zHe+8x2uvPLKY/qxIc5AF5GLgR8CXuAhVf1uv+e/D5wbncwBJqhq0TFXdxh2gi5jzFBdeeWVfOUrX9kf6IsWLWLp0qXceuutFBQU0NzczGmnncZll112VBdqfuCBBxAR3nnnHd59910uvPBCNmzYwE9/+lO+/OUvc8011xAMBgmHwyxZsoTJkyfz9NNPA9DW1paQn23QQBcRL/AAcAFQD6wQkcWqurZvGVX9aszytwAnJ6S6wyjMsUA3Ji0coSU9XE4++WQaGxt5//33aWpqori4mLKyMr761a/y4osv4vF4aGhoYPfu3ZSVlcX9vsuXL+eWW24BYPbs2UydOpUNGzZw+umnc++991JfX88nPvEJZsyYQU1NDV//+tf5xje+wUc/+lHOOuushPxs8fShLwA2qepmVQ0CjwNH+i5yNe66osPGWujGmGPxqU99iieffJInnniCK6+8kscee4ympiZWrlzJqlWrmDhxIoFAICHr+vSnP83ixYvJzs7m0ksv5fnnn2fmzJm88cYb1NTU8O1vf5u77747IeuKp8ulHNgRM10PfGCgBUVkKlANPH/spR1eboYXr51C1xgzRFdeeSU33HADzc3N/OUvf2HRokVMmDABv9/PsmXL2LZt21G/51lnncVjjz3Geeedx4YNG9i+fTuzZs1i8+bNTJs2jVtvvZXt27fz9ttvM3v2bMaNG8dnPvMZioqKeOihhxLycyV6p+hVwJOqGh7oSRG5EbgRYMqUKUNeiZ1C1xhzLE444QT27dtHeXk5kyZN4pprruFjH/sYNTU11NbWMnv27KN+zy996Ut88YtfpKamBp/PxyOPPEJmZiaLFi3iF7/4BX6/n7KyMr75zW+yYsUKbrvtNjweD36/n5/85CcJ+blE9chHW4rI6cBdqnpRdPoOAFU95OzuIvImcJOq/nWwFdfW1mpdXd2QigY4999f4ITJBfzo06cM+T2MMSNv3bp1HH/88ckuIyUM9LsSkZWqWjvQ8vH0oa8AZohItYhk4Frhi/svJCKzgWLglaOueggKrIVujDEHGbTLRVV7ReRmYClu2OLDqrpGRO4G6lS1L9yvAh7XwZr8CVKY7aetKzgSqzLGjHHvvPMO11577UHzMjMzee2115JU0cDi6kNX1SXAkn7z7uw3fVfiyhpcYbaf7S2dI7lKY0yCqOpRjfFOtpqaGlatWjWi6xxK2zglD/0HKMz2WZeLMSkoKyuLlpaWIQXWWKGqtLS0kJWVdVSvS8lD/yF6Ct1Ab8pt6Y0Z6yoqKqivr6epqSnZpYxqWVlZVFRUHNVrUjrQwxGlo6eX/Cx/sssxxsTJ7/dTXV2d7DLSUgp3udjRosYYE8sC3Rhj0kTKBrqdE90YYw6WsoFu50Q3xpiDpXygWwvdGGMcC3RjjEkTKRvoeZk+PGKBbowxfVI20EXETtBljDExUi/QNzwDv/kCRCLRc6L3JrsiY4wZFVIv0Pdug3d+DV3NdpELY4yJkXqBXhg9t0HbDgt0Y4yJkcKBXk9Btt/GoRtjTFRKB7q10I0x5oDUC/SsIsjIg70HulzsvMrGGBNnoIvIxSKyXkQ2icjth1nmChFZKyJrROSXiS3zoBW5Vnq0Dz0cUTqD4WFbnTHGpIpBz4cuIl7gAeACoB5YISKLVXVtzDIzgDuAD6pqq4hMGK6CgWig11M47cDRonmZKXtqd2OMSYh4WugLgE2qullVg8DjwMJ+y9wAPKCqrQCq2pjYMvsprNzfhw7Q1mX96MYYE0+glwM7Yqbro/NizQRmisjLIvKqiFw80BuJyI0iUicidcd0+anCCuhqptjvulpsx6gxxiRup6gPmAF8CLga+B8RKeq/kKo+qKq1qlpbWlo69LUVVgJQEnYbBQt0Y4yJL9AbgMqY6YrovFj1wGJVDanqFmADLuCHR3ToYlFoNwDtAQt0Y4yJJ9BXADNEpFpEMoCrgMX9lnkK1zpHRMbjumA2J7DOg0UDPS+wE7CLXBhjDMQR6KraC9wMLAXWAYtUdY2I3C0il0UXWwq0iMhaYBlwm6q2DFfRFEwGhOyu9xE7ha4xxgBxDFsEUNUlwJJ+8+6MeazA16K34ef1Q/4kpL2Bgqz5FujGGEMqHinap6jSTtBljDExUjfQ+w4uskA3xhgg5QO9gaIsrwW6McaQ0oFeCeEeyjM6LdCNMYaUDnQ3dLHSu8eGLRpjDGkQ6JNpslPoGmMMaRDoE7SZUFjpDtkpdI0xY1vqBnpWEWTkU9LrTuxo/ejGmLEudQM9eqGLvvO5WKAbY8a61A10gMIKcqPnc7FzohtjxrqUD/TsrmigWwvdGDPGpXyg+wItZBK0QDfGjHkpHujuNO3l0myBbowZ81I80N3QxXJpsYOLjDFjXmoHepFroU/LaLUWujFmzEvtQM+fBOKhyreHvRboxpgxLq5AF5GLRWS9iGwSkdsHeP56EWkSkVXR2xcSX+oAohe6qPK3srMtMCKrNMaY0WrQKxaJiBd4ALgAdzHoFSKyWFXX9lv0CVW9eRhqPLLCCspbWmho7R7xVRtjzGgSTwt9AbBJVTerahB4HFg4vGUdhcIKSiNN7GzrJhSOJLsaY4xJmngCvRzYETNdH53X3+Ui8raIPCkilQO9kYjcKCJ1IlLX1NQ0hHIHUFhBQXAXqhF2WbeLMWYMS9RO0T8AVap6IvAs8POBFlLVB1W1VlVrS0tLE7Pmwkq8kRDjaWdHa1di3tMYY1JQPIHeAMS2uCui8/ZT1RZV7YlOPgScmpjy4hA9uGiyNFNv/ejGmDEsnkBfAcwQkWoRyQCuAhbHLiAik2ImLwPWJa7EQUQPLqrwtFigG2PGtEFHuahqr4jcDCwFvMDDqrpGRO4G6lR1MXCriFwG9AJ7gOuHseaDRQN9VlYbW63LxRgzhg0a6ACqugRY0m/enTGP7wDuSGxpccoqhIx8pvv2stxa6MaYMSy1jxQFd6GL4qlMk502Ft0YM6alfqADlJ3IlOBGdrZ12Vh0Y8yYlR6BPnkeuaE9jNe9NhbdGDNmpUegTzoJgBrPZhuLbowZs9Ij0MtqUIS5stWGLhpjxqz0CPSMXBg/kxrPFgt0Y8yYlR6BDsjkeZzo3Uq9dbkYY8aotAl0Jp3EBPawr/n9ZFdijDFJkVaBDlDQuibJhRhjTHKkT6CXnQjA5O71NhbdGDMmpU+gZxWwL3cqJ8hWG4tujBmT0ifQgcD4GuZ6tthYdGPMmJRWge6rmEeFNNO4e2eySzHGmBGXVoGeVzUfgN6GVUmuxBhjRl5aBbq/wo10yWp6O8mVGGPMyEurQCe7mF3eMkraR+6CScYYM1rEFegicrGIrBeRTSJy+xGWu1xEVERqE1fi0dmVM4spPRuStXpjjEmaQQNdRLzAA8AlwBzgahGZM8By+cCXgdcSXeTR2Fd8AuW6m1DHnmSWYYwxIy6eFvoCYJOqblbVIPA4sHCA5f4Z+DcgqYPAw2WuH731vbpklmGMMSMunkAvB3bETNdH5+0nIqcAlar6dAJrG5LsKacA0LXtjSRXYowxI+uYd4qKiAe4H/h6HMveKCJ1IlLX1NR0rKseUNmkcup1PJ5dNnTRGDO2xBPoDUBlzHRFdF6ffGAu8IKIbAVOAxYPtGNUVR9U1VpVrS0tLR161UcwqTCbNZEq8vbYSbqMMWNLPIG+ApghItUikgFcBSzue1JV21R1vKpWqWoV8CpwmaompRM7w+dha8YMxgW2Q6A9GSUYY0xSDBroqtoL3AwsBdYBi1R1jYjcLSKXDXeBQ9FSMNs9eP/N5BZijDEjyBfPQqq6BFjSb96dh1n2Q8de1rFpL51Pb6sX33vPw7Rzkl2OMcaMiPQ6UjSqdPx4VkRmoRufSXYpxhgzYtIy0CuKs3k+PA9pXAtt9ckuxxhjRkSaBnoOyyLz3IS10o0xY0RaBvqcSQVs0nLaMyfBxmeTXY4xxoyItAz04twMppfmUZcxHza/AL09yS7JGGOGXVoGOkDt1HH8rmMOhLpg28vJLscYY4Zd2gb6qVOLebZ7FhFvpnW7GGPGhLQN9FOmFhMgk93jamHD0mSXY4wxwy5tA316aS5FOX5e89XCnveg5b1kl2SMMcMqbQNdRDh1SjFPtkevxWHdLsaYNJe2gQ6u22V5Sz7hccfZeHRjTNpL60CvnVoMQMP4M2Hrcgh2JrkiY4wZPmkd6CdWFOHzCK94T4VwD2x5MdklGWPMsEnrQM/O8HLC5AIWt06FjDzrdjHGpLW0DnRw/egrG7qIVJ8D6/8E4d5kl2SMMcMi7QO9duo4AqEI2yoWwr6dsOFPyS7JGGOGRdoH+ilTiwB4QU+BggpY8VCSKzLGmOERV6CLyMUisl5ENonI7QM8/3ci8o6IrBKR5SIyJ/GlDs2kwmzKi7Kpq98Htde7k3U1b0x2WcYYk3CDBrqIeIEHgEuAOcDVAwT2L1W1RlXnAd8D7k94pcfg1KnFrNzaip78WfD4rZVujElL8bTQFwCbVHWzqgaBx4GFsQuoanvMZC6giSvx2J06tZhd7QHeDxfAnIWw6pc2Jt0Yk3biCfRyYEfMdH103kFE5CYReQ/XQr91oDcSkRtFpE5E6pqamoZS75CcGj3AaOW2VlhwA/S0w9uLRmz9xhgzEhK2U1RVH1DV6cA3gG8fZpkHVbVWVWtLS0sTtepBzS7LJyfDy8qte6DyAzBxrut20VH1RcIYY45JPIHeAFTGTFdE5x3O48DfHEtRiebzephXWcSKra0gAvO/ALtXw47Xkl2aMcYkTDyBvgKYISLVIpIBXAUsjl1ARGbETH4EGHXDSD543HjW7mzn/b3dUPMpyCywnaPGmLQyaKCrai9wM7AUWAcsUtU1InK3iFwWXexmEVkjIquArwHXDVvFQ3TJ3DIA/rx6F2TmwbxPw5qnoKMxyZUZY0xixNWHrqpLVHWmqk5X1Xuj8+5U1cXRx19W1RNUdZ6qnquqa4az6KGYVprH7LJ8F+jgul0iIah7OLmFGWNMgqT9kaKxLpk7iRXb9tDYHoDxM2DmJfDqTyDQPviLjTFmlBtbgV5ThiosXRNtpZ/zDxDYC68/mNzCjDEmAcZUoM+YkMf00lz+1NftUn4KzLgQXvkR9OxLbnHGGHOMxlSgiwiX1kzi1c0ttHT0uJnnfAO6W2HF/ya3OGOMOUZjKtABLp5bRkThmbW73YyKWpj+Yfjrf9npAIwxKW3MBfqcSQVMLck50O0CrpXe1WwjXowxKW3MBbqIcMncSfx1UzN7u4Ju5pQPwLQPwcv/CcGuZJZnjDFDNuYCHdxBRr0R5dm+bhdwrfTORlj5SNLqMsaYYzEmA/3EikLKi7IPHGQEMPUMqDoLXv6BtdKNMSlpTAa663Yp46WNzbQHQgeeOPdb0LHbhboxxqSYMRnoAJfUTCIYjvDcuphul6mnw9xPwvIfwJ4tySvOGGOGYMwG+smVRVSOy+axV7cf/MSF/wweHyz9VnIKM8aYIRqzge7xCNefUU3dtlbe2rH3wBMFk+Gc22D907Dx2eQVaIwxR2nMBjrAFbUV5GX6+NnL/bpXTvsSjJsOf/oG9PYkpzhjjDlKYzrQ87P8XFFbyR/f3snu9sCBJ3yZcMn3YM978OqPk1egMcYchTEd6ADXn1FFWJVHX9l68BMzzodZl8Jf7oP295NRmjHGHJW4Al1ELhaR9SKySURuH+D5r4nIWhF5W0SeE5GpiS91eEwpyeGC4yfyy9e20x0MH/zkRf8CkV740z/YBaWNMaPeoIEuIl7gAeASYA5wtYjM6bfYm0Ctqp4IPAl8L9GFDqfPn1lNa1eIp1b1u/b1uGo495uw7g/wxs+TU5wxxsQpnhb6AmCTqm5W1SDwOLAwdgFVXaaqfYdXvgpUJLbM4bWgehwnTC7g4eVb0P4t8TNuhWnnuh2kjeuSU6AxxsQhnkAvB3bETNdH5x3O54E/DfSEiNwoInUiUtfU1BR/lcNMRPjbD1azsbGDlzY2H/ykxwOfeBAyC+DXn7PTAhhjRq2E7hQVkc8AtcB9Az2vqg+qaq2q1paWliZy1cfsoydNYnxeJg/3H8IIkDcBPvHf0LQOlt4x8sUZY0wc4gn0BqAyZroiOu8gInI+8C3gMlVNucHbmT4vnz19Ki+sb2J1Q9uhC0w/D878qjsb45rfjXh9xhgzmHgCfQUwQ0SqRSQDuApYHLuAiJwM/DcuzBsTX+bIuO6MKopz/Nzz9NpD+9LBnbyrYj4svhX2bB75Ao0x5ggGDXRV7QVuBpYC64BFqrpGRO4Wkcuii90H5AG/FpFVIrL4MG83qhVm+/nK+TN5dfMenls3wHbJ64fL/xc8XnjsCnctUmOMGSVkwJboCKitrdW6urqkrPtIQuEIF/3gRQCWfuVs/N4Btnnb/gqPLoTKD8Bnfgu+jBGu0hgzVonISlWtHei5MX+kaH9+r4c7LjmezU2d/Or17QMvNPUMWPhj2PoS/OFWO+jIGDMqWKAP4PzjJ3DatHH84P82HnwBjFgnfsr1qb/1K3hxwEE9xhgzoizQByAifPsjc2jtCvLjZe8dfsGzb4OTroZl98Lbi0auQGOMGYAF+mHMLS/k4yeX8/DLW9ix5zAHE4nAx/7TXYv0d38Hbz42skUaY0wMC/QjuO2iWXgE/mXJuoGHMYLbIXr1r6D6bPj9l1z3i/WpG2MGEgq4b/NthxzKkxAW6EcwqTCbWz88gz+t3nXoibtiZebDpxdBzRXw/D2w5O8hEj788saYsaV5k7us5f3Hw29vgNVPDstqfMPyrmnk/509nWXvNnLnU2uYXzWOiuKcgRf0ZcDH/xvyy+Cv/wn7dsHlD4E/e2QLNsYMjSr0BiDQBoF2CPdAOOQaZ5EQeDOhuApyxrnu1sG0boMtf3Et8q0vuWsVz/4InPo5qD5nWH4EG4cehx17urjkhy8xZ3IBv7rhNLyeQf6Yr/4E/nwHlBwHH/kPmDY8fzxjzFEKBaB5PTRvjN42QMtG1wALtEE4OPh7ZBa4YB9XDXkTIasIsovcPcD2V2DLi7B3m5sumgKnXAcnXwv5E4/5RzjSOHQL9Dg9ubKev//1W9x+yWz+7pzpg79g03Pw9NegdSvUfAouvDchf0xjTJxUYcfrsONV2PUO7FrtAlz7ukPFhe34mVBY7gI5q/DAzZvhjg73+NzR4aFu9/+8Z4u7b90CnU2uNU9MjmYVuoES1edA9VlQOju+Fn2cjhTo1uUSp8tPKee5dbv5j2fWc9aM8ZwwufDILzjuw/ClV2H5991tw1I47zsw//Puw2GMiU+wC9573oVnZj5k5EJGngvOCXPAO0CMNayE/7vLtZQBCiqgbC4c/1GYeAKMnwXjpoE/69jri0Sgpx0Ce6E3CCXTk/Y/bi30o9DaGeSiH7xIYbafP9xyJln+OP9ozZtgyddh8wtQVgOX/jtMOW1YazUmZage2oINdsLGZ2Dt72HDMxDqHPi1WYUw/cMw40KYcYE7v9Jzd8O6xZBT4o4VOfFK1++dJqzLJYH+sqGJ6x5+nQvnTOTH15yCb6BzvQxEFdY+5fZ0tze4A5LO/yfrhjFjU6gbNj4bDeylLrC9GQe6OYKdbgdlbikc/zGYsxBKZrj5wX3uvqMRNi9z79OxGxC3YfDnwBm3wOk3uRZ9mrFAT7CHl2/h7j+u5VOnVvC9T56IHE3/WLATXvx3+Ot/gS8LPvhlOOVaNzrGmFSk6j7X3a2QmQfZxYcuEwm7/uuGN2DT/x0I8ZwSmHWp+/yHg25USTjo/jdmXgRTPzh490UkArvedi36cBAW/D/IG10X0EkkC/RhcP+zG/jP5zZyw1nVfPPS448u1CE6LvWbsHEpiNd9eE++1n11HKhP0JiR0t0K21+FxrXuOrqN77qRIIjrc/bnuOG44oHuvW75SMw5j7IK3SiQoqmQO969fudbB7pNckqire6/cTsP7fN+VGyn6DD46vkzaOsK8j8vbaEoJ4Obzj3u6N5g/HFwzSJoeQ/e/IU7bcD6JW4Y1JTTYOJct8Nn4gnuH8Njx4CZo7RrteuSaN0Ge7e7W3sDTDkdLvxnKJ118PKRCLz5KDz7j24HH0BhpRulMe2cAyM9Ql3uPtLrWuN9t6wit3OwdZsbBdK4Djob3etPuRYmn+xuJTPs8zxMrIV+DCIR5auLVvH7Ve/zz38zl2tPmzr0NwuH3FfGtxe51kzrVvYPhfLnug3A+FlQOtPdTzgeiqutdWMO1tvj+qVX/K8brgeuxVw0xTUMckpgzVMQ7ID5X4AP3e52GO5aDX/8KtS/DlPPhHO/6XbgZxUk9+cxhzjmLhcRuRj4IeAFHlLV7/Z7/mzgB8CJwFWqOuhxrekQ6OAuiPF3v1jJc+82cvO5x/G1C2biGezAo3j0dEDTu7B7jWvpNK+Hpg3QXn9gGW+mC/gJc1zAj5/lWl3FVTY0cqzpbIFXfgRvPApdzTBuuhsiW3PFof3Jnc2w7F9g5c/cQTIzLoDVv3UHx1x4L5x0VULHTZvEOqZAFxEvsAG4AKjHXWP0alVdG7NMFVAA/D2weCwFOkCwN8J3nlrNE3U7uOiEidx/xTxyM4ep5dzT4XYuNa2P6eNc1y/oM9xRquOmuZ1NeWXuPn8STJwDBZOHpzYz8oKd8OqPYfkPXR/1rEtdkFd/aPBujd1rYekdbjjtKZ91o67SaHhfujrWPvQFwCZV3Rx9s8eBhcD+QFfVrdHnIsdcbQrK8Hn47uU1zCrL556n1/LJn77CQ9fVUl40DOdxycyD8lPcLVagzbXgm9e7sG/e4Prnty4/0B/aJ38yVJwK5bXuKLlgx4GdW33LZua7gzcy893X7oJy15+aX2at/9atsOUl14eckRc90CXX/a5yStyOwIzcg18TDrm/Udce14/dVn/gvmffgR2N/mz32qqzoOrMw7eUw72uv/uF77ohe7M+Auf/46H94kcycQ5c+5TrpknEATYm6eIJ9HJgR8x0PfCBoaxMRG4EbgSYMmXKUN5i1BIR/vbMaqZPyOPmX77Bwh8t5/4r5nH2zBEaPpVVCJXz3a2/UMD907c3uP75+jpoqIN1fzh02Yw8dx/sGHg9Hp8L9/yyaADluCFm/oJws4cAAA59SURBVOx+QS/um0LBZNd/W1jp7sM90LLJjfJp2eQCrXSWC68ppx153HCwC7r3uFDsaXejLDw+N0rIE731PRaPq6Gz0Z2qtD166+mA3BK38zl3guuO8GW5YXUacTfU1e7LAl+mu2/d5obbbXrW1T0YX7YLdlW3kTzc7zNvovvbhQIHdjaGuuAv/+a60hbc4A6Mych1G4VtL7u/27o/Qscud13bKx4d+oFqIhbmaSSeLpdPAher6hei09cCH1DVmwdY9hHgj2Oty6W/TY0d3PhoHZubOzlv9gS+eenxHDchL9llHaqz2QVVVkF0lEKhO6gD3IiHUKdrPQbaXCju3QZtO2DvDncYdm9sCHVHw5AD54Pv7XavPZzMAhf4Le+5YW/ihUknuYDv2ee+MXTvdYHYtce937HILHTfcDqb3YblaPmyXMv5uPNh+rmu/mCnC+tgZ7QF3uL6sDub3WMkOgokevKm7GL3MxeWu29KA11gPNQN7zwJr/+3OwdJZqE7J8i2l93vxJcNM86HedfAzIutv3uMOdY+9NOBu1T1ouj0HQCq+q8DLPsIFugA9PSGeeTlrfzX85sIhMJ85rSpfOX8GRTlDPAPnM56Og5sBNq2g8cP42e4Pv7cUhdGwU53EqVtL8PWl92Go+8Mdn3D4XKKIXuc69LIGefCFHXdHpFw9NbrTrykEbdBQt3yhRUuRPta/6quhd/R5Frw4aBr0Utfy57oQS5Bt9Hq7XHrnHL6yJ4OWRV2vAavPwjbXnGhfvzH3KHuGYc5jbNJe8ca6D7cTtEPAw24naKfVtU1Ayz7CBboB2nu6OH+Zzfw+Ovbycv0cf0ZVXz2jCrG52UmuzRjTApKxLDFS3HDEr3Aw6p6r4jcDdSp6mIRmQ/8DigGAsAuVT3hSO85VgK9z7u72vmPZzbw7NrdZPo8XFFbyQ1nTWNKibW0jDHxs0P/R5FNjR38z4ub+d2bDfRGIpx//EQ+fnI5586eEP/ZG40xY5YF+ii0uz3Az17eypMr62nu6CE/y8clc8v4m3nlLKgeF/9ZHI0xY4oF+ijWG47w1/da+P2q91m6ZhcdPb0U5fg5Z2Yp582ewIdmTqAwx5/sMo0xo4QFeooIhMI8/24jz61r5IX1jbR0BvF6hJMri5hfPY75VcWcOmWcBbwxY5gFegoKR5S36vey7N1GXtrYzOqGNnoj7m81a2I+J1UWMmdSAXMmFzJ7Uj4FWRbyxowFFuhpoDsY5q36vdRt3cOKra2sbmijpfPAFcorx2Uzu6yA48vymVVWwOxJ+VSV5OJNxInCjDGjhp0PPQ1kZ3g5bVoJp00rAUBVadrXw5qd7ax9393e3dXOc+t2E23Ik+nzML00j5kT85hZls/MCflMK82lojiHDJ/tdDUm3VigpygRYUJBFhMKsjh31oT98wOhMBt3d7BuVzsbdu1jQ2MHr23Zw1Or3t+/jNcjlBdlM7Ukh6qSXKaW5FA5Locp49x93nCdKdIYM6zsPzfNZPm91FQUUlNReND89kCIjbs72NrcybaWTra0dLGtpZOnVjWwL9B70LIluRn7A77vVlaYxcSCLCYWZFKY7T/6S+4ZY4adBfoYUZDl59SpxZw69dAL+LZ1hdi+p4ttezrZ1tJFfWsX2/d08eaOVp5+ZyfhyMH7WTJ8HiYWZDKpMJvJhVmUFWYzuSiLCflZlOZnMiE/k/F5mWRn2IFSxowkC3RDYY6fmpxDW/Xgrsi0qy3A7vYAu9oD7G7vobE9wM62ALvaAtRta2V3+05C4UN3rudmeMnP8pOX5SMv00d+lo/inAxK8zPdLS+T8fmuxV+Q5SM/y09+ls+OmDVmiCzQzRH5vR4qo33rhxOJKM2dPTTti7l19NDSEWRfIERHTy/7Au62fU8Xje09dIfCh32/nAwvkwqzmFyUTXlRNpOLsinJy6Aw27//VpTtpvOzfIm55J8xacAC3Rwzj0eYkO+6XOLV2dO7P/jbu0PRwA/RHuilpSPIzrZu3t/bzbqd+2juOPy5y0Vcd1Jhtp+iHD9FORkUZfsp7nucE52fnUFhjp+CLD9Zfg/Zfi9Z0ZsN7TTpwgLdJEVupo/cTB9V43MHXTYQCrO3K0Rb9wC3riBt3SH2dofY2+Xut7V00toZpL3fzt7D1pLhpTg3g+KcjOi9n7xM103UV2duRt8GwLN/Q9DXjdTXVeS38++YJLNAN6Nelt9LWaGXssKju1RaOKK07w/7IHu7Q7R3h+gJRQj0hukOhgmEIrQHQrR2BtnTFaS1M8iW5g46e8J09vTS0xv/ZXIzfZ794d4X9jnRDUHfN4LsDPc4J8PdsjPcxiInutHIyfCRm+kl0+clw+fB7xUyfB4yvB4bWWQGZYFu0pbXI67FnZsBDP5NYCChcISunjCdwV4CIbcB6A6FCYTC+/cNdASiXUZ90z2u+2hfoJfWrhA90eUDvRG6g+Ej7j84kkyfh0zfgW8IORlet0M5ul+hIMtPbqbbKPRtMPq6lDwieMT9TlQhrEokooRVEYSiHD/FORmU5LlvKnbgWWqyQDfmCPxeD4U5noSeEC0SUbpDYbqCYbqCvXT2hOkOufu+eT29EYJ9t3CEnlCYnt4IgZj7jp4w7YEQO/Z0sSbaBdUVCpOIs3n07WfI9nvJivlW0feNIju6wcj0uW6oTJ+XTJ+HDJ9n/7eLvm8Wfq/g93rwRe8zvB4yY17T97q+52wn99BZoBszwjwe2d83D4m9FKGqEghF6Az2RruUwtHWOERUCUcUEfCI4PXI/hZ7a7S7qaUzyJ5ONzqp79tIdyhMIOg2Nm3dIXa1ddPZE96/cenpDQ84bHWovB7ZvxHwez34PO6x+6bhau/7Gfq6s/q6srL8BzYOfq/bWPQ97uvC6psXu5zfK/ii6/J5BJ9X8HncRijD68EXu2HyyP7pvloAhAPfgpLVPRZXoIvIxcAPcZege0hVv9vv+UzgUeBUoAW4UlW3JrZUY8xgRMSF2wgf1BWOKIFQeP83imA06Ht6I/SGld5IhGBv333f826ZQChCKOxeF+pVgmG3gQiF3fzesBIMR4hEFAUi6jZOkeg6u0Nh9nYF2Rl93Bt9bV8tobAecnDccMuI2Uj4oxuHvg2V1yN85fyZfOykyQlf76CBLiJe4AHgAqAeWCEii1V1bcxinwdaVfU4EbkK+DfgyoRXa4wZlbz7v3Uku5KBhSMu5Pu6svo2Fn3z3EZH6Q1H3LLRx30blt6I29iEIpH9G4zeyMEbClUlou6iNaGIEupbT0QJh91rwxGlN6wUDdM1DeJpoS8ANqnqZgAReRxYCMQG+kLgrujjJ4EfiYhoss7Na4wxMVz3kjftj0KOZ1d2ObAjZro+Om/AZVS1F2gDSvq/kYjcKCJ1IlLX1NQ0tIqNMcYMaETHJqnqg6paq6q1paWlI7lqY4xJe/EEegNQGTNdEZ034DIi4gMKcTtHjTHGjJB4An0FMENEqkUkA7gKWNxvmcXAddHHnwSet/5zY4wZWYPuFFXVXhG5GViKG7b4sKquEZG7gTpVXQz8L/ALEdkE7MGFvjHGmBEU1zh0VV0CLOk3786YxwHgU4ktzRhjzNGwEzYYY0yasEA3xpg0IcnadykiTcC2Ib58PNCcwHJGSqrWDalbu9U9sqzu4TdVVQcc9520QD8WIlKnqrXJruNopWrdkLq1W90jy+pOLutyMcaYNGGBbowxaSJVA/3BZBcwRKlaN6Ru7Vb3yLK6kygl+9CNMcYcKlVb6MYYY/qxQDfGmDSRcoEuIheLyHoR2SQitye7nsMRkYdFpFFEVsfMGyciz4rIxuh9cTJrHIiIVIrIMhFZKyJrROTL0fmjunYRyRKR10XkrWjd/xSdXy0ir0U/L09ETzA36oiIV0TeFJE/RqdHfd0islVE3hGRVSJSF503qj8nACJSJCJPisi7IrJORE5PhbrjkVKBHnM5vEuAOcDVIjInuVUd1iPAxf3m3Q48p6ozgOei06NNL/B1VZ0DnAbcFP0dj/bae4DzVPUkYB5wsYichrsc4vdV9TigFXe5xNHoy8C6mOlUqftcVZ0XM4Z7tH9OwF0f+c+qOhs4Cfd7T4W6B6eqKXMDTgeWxkzfAdyR7LqOUG8VsDpmej0wKfp4ErA+2TXG8TP8Hnc92ZSpHcgB3gA+gDv6zzfQ52e03HDXGHgOOA/4IyApUvdWYHy/eaP6c4K7VsMWogNCUqXueG8p1UInvsvhjWYTVXVn9PEuYGIyixmMiFQBJwOvkQK1R7stVgGNwLPAe8BedZdFhNH7efkB8A9AJDpdQmrUrcAzIrJSRG6Mzhvtn5NqoAn4WbSL6yERyWX01x2XVAv0tKGuKTBqx4yKSB7wG+Arqtoe+9xorV1Vw6o6D9fiXQDMTnJJgxKRjwKNqroy2bUMwZmqegquC/QmETk79slR+jnxAacAP1HVk4FO+nWvjNK645JqgR7P5fBGs90iMgkget+Y5HoGJCJ+XJg/pqq/jc5OidoBVHUvsAzXVVEUvSwijM7PyweBy0RkK/A4rtvlh4z+ulHVhuh9I/A73EZ0tH9O6oF6VX0tOv0kLuBHe91xSbVAj+dyeKNZ7KX6rsP1T48qIiK4K1CtU9X7Y54a1bWLSKmIFEUfZ+P6/dfhgv2T0cVGXd2qeoeqVqhqFe7z/LyqXsMor1tEckUkv+8xcCGwmlH+OVHVXcAOEZkVnfVhYC2jvO64JbsTfwg7NS4FNuD6R7+V7HqOUOevgJ1ACNcq+Dyub/Q5YCPwf8C4ZNc5QN1n4r5uvg2sit4uHe21AycCb0brXg3cGZ0/DXgd2AT8GshMdq1H+Bk+BPwxFeqO1vdW9Lam739xtH9OojXOA+qin5WngOJUqDuemx36b4wxaSLVulyMMcYchgW6McakCQt0Y4xJExboxhiTJizQjTEmTVigG2NMmrBAN8aYNPH/Ab2RJepstgzVAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "lossdf = pd.DataFrame(ann.history.history)\n",
        "lossdf.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "drKVEIj2Zlap"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "QjoEouTqaBNy"
      },
      "outputs": [],
      "source": [
        "ann = Sequential()\n",
        "\n",
        "ann.add(Dense(units=30, activation=\"relu\"))\n",
        "ann.add(Dropout(rate=0.5)) #Drop 50% neurons\n",
        "\n",
        "ann.add(Dense(units=15, activation=\"relu\"))\n",
        "ann.add(Dropout(rate=0.5))\n",
        "\n",
        "ann.add(Dense(units=1, activation=\"sigmoid\"))\n",
        "\n",
        "ann.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "ucMoVxLxa9Dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8917f940-51ed-4d3a-948b-9250b25ef7a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/600\n",
            "15/15 [==============================] - 1s 14ms/step - loss: 0.8898 - val_loss: 0.6150\n",
            "Epoch 2/600\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.6646 - val_loss: 0.4901\n",
            "Epoch 3/600\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.5050 - val_loss: 0.4094\n",
            "Epoch 4/600\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.4793 - val_loss: 0.3446\n",
            "Epoch 5/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4129 - val_loss: 0.2906\n",
            "Epoch 6/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.3630 - val_loss: 0.2476\n",
            "Epoch 7/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.3197 - val_loss: 0.2154\n",
            "Epoch 8/600\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.2711 - val_loss: 0.1928\n",
            "Epoch 9/600\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.2689 - val_loss: 0.1768\n",
            "Epoch 10/600\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.2183 - val_loss: 0.1629\n",
            "Epoch 11/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.2432 - val_loss: 0.1518\n",
            "Epoch 12/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.2011 - val_loss: 0.1432\n",
            "Epoch 13/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.2121 - val_loss: 0.1358\n",
            "Epoch 14/600\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.2004 - val_loss: 0.1287\n",
            "Epoch 15/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.1621 - val_loss: 0.1248\n",
            "Epoch 16/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.1887 - val_loss: 0.1223\n",
            "Epoch 17/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.1555 - val_loss: 0.1186\n",
            "Epoch 18/600\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 0.1446 - val_loss: 0.1156\n",
            "Epoch 19/600\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1592 - val_loss: 0.1121\n",
            "Epoch 20/600\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.1298 - val_loss: 0.1114\n",
            "Epoch 21/600\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.1246 - val_loss: 0.1081\n",
            "Epoch 22/600\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1442 - val_loss: 0.1049\n",
            "Epoch 23/600\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1274 - val_loss: 0.1016\n",
            "Epoch 24/600\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1197 - val_loss: 0.1005\n",
            "Epoch 25/600\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.1320 - val_loss: 0.1001\n",
            "Epoch 26/600\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.1051 - val_loss: 0.1001\n",
            "Epoch 27/600\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1258 - val_loss: 0.1008\n",
            "Epoch 28/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0899 - val_loss: 0.1030\n",
            "Epoch 29/600\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 0.1084 - val_loss: 0.1033\n",
            "Epoch 30/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.1105 - val_loss: 0.1019\n",
            "Epoch 31/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0890 - val_loss: 0.1021\n",
            "Epoch 32/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.1194 - val_loss: 0.1019\n",
            "Epoch 33/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0897 - val_loss: 0.1022\n",
            "Epoch 34/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0844 - val_loss: 0.1018\n",
            "Epoch 35/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0849 - val_loss: 0.1013\n",
            "Epoch 36/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.1006 - val_loss: 0.1017\n",
            "Epoch 37/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.1160 - val_loss: 0.0994\n",
            "Epoch 38/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.1086 - val_loss: 0.1007\n",
            "Epoch 39/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0849 - val_loss: 0.1009\n",
            "Epoch 40/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0781 - val_loss: 0.1008\n",
            "Epoch 41/600\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 0.0953 - val_loss: 0.0996\n",
            "Epoch 42/600\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 0.0953 - val_loss: 0.0999\n",
            "Epoch 43/600\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 0.0919 - val_loss: 0.1006\n",
            "Epoch 44/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0809 - val_loss: 0.1007\n",
            "Epoch 45/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0878 - val_loss: 0.1015\n",
            "Epoch 46/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0853 - val_loss: 0.1018\n",
            "Epoch 47/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0896 - val_loss: 0.1028\n",
            "Epoch 48/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0953 - val_loss: 0.1009\n",
            "Epoch 49/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0823 - val_loss: 0.0998\n",
            "Epoch 50/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0603 - val_loss: 0.0992\n",
            "Epoch 51/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0942 - val_loss: 0.0998\n",
            "Epoch 52/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0724 - val_loss: 0.1008\n",
            "Epoch 53/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0976 - val_loss: 0.1031\n",
            "Epoch 54/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0770 - val_loss: 0.0994\n",
            "Epoch 55/600\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 0.0996 - val_loss: 0.0987\n",
            "Epoch 56/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0559 - val_loss: 0.0971\n",
            "Epoch 57/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0751 - val_loss: 0.0968\n",
            "Epoch 58/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0811 - val_loss: 0.0973\n",
            "Epoch 59/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0681 - val_loss: 0.0952\n",
            "Epoch 60/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0605 - val_loss: 0.0934\n",
            "Epoch 61/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0563 - val_loss: 0.0927\n",
            "Epoch 62/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0570 - val_loss: 0.0930\n",
            "Epoch 63/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0647 - val_loss: 0.0940\n",
            "Epoch 64/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0604 - val_loss: 0.0944\n",
            "Epoch 65/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0793 - val_loss: 0.0946\n",
            "Epoch 66/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0477 - val_loss: 0.0959\n",
            "Epoch 67/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0638 - val_loss: 0.0965\n",
            "Epoch 68/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0673 - val_loss: 0.0963\n",
            "Epoch 69/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0571 - val_loss: 0.0977\n",
            "Epoch 70/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0859 - val_loss: 0.0991\n",
            "Epoch 71/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0520 - val_loss: 0.0976\n",
            "Epoch 72/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0568 - val_loss: 0.0965\n",
            "Epoch 73/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0605 - val_loss: 0.0966\n",
            "Epoch 74/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0500 - val_loss: 0.0975\n",
            "Epoch 75/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0507 - val_loss: 0.0985\n",
            "Epoch 76/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0432 - val_loss: 0.0991\n",
            "Epoch 77/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0497 - val_loss: 0.0987\n",
            "Epoch 78/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0618 - val_loss: 0.0970\n",
            "Epoch 79/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0441 - val_loss: 0.0971\n",
            "Epoch 80/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0678 - val_loss: 0.0962\n",
            "Epoch 81/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0485 - val_loss: 0.0970\n",
            "Epoch 82/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0504 - val_loss: 0.0986\n",
            "Epoch 83/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0518 - val_loss: 0.0990\n",
            "Epoch 84/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0511 - val_loss: 0.1007\n",
            "Epoch 85/600\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0526 - val_loss: 0.1027\n",
            "Epoch 86/600\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0543 - val_loss: 0.1028\n",
            "Epoch 86: early stopping\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f49da4a8910>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "ann.fit(xtrain, ytrain, epochs=600, validation_data=(xtest, ytest), callbacks=[earlystop])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "EMjzCmasbILI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "a9f3abe6-1111-4db5-bba3-957a76841fc6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f49da1ce5b0>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "lossdf = pd.DataFrame(ann.history.history)\n",
        "lossdf.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "8HHU0cSKbJLl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5454747e-00e4-45ca-998b-6d8971c9b00f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 0s 3ms/step\n"
          ]
        }
      ],
      "source": [
        "ypred = ann.predict(xtest)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "Tn1p4_KTbYI_"
      },
      "outputs": [],
      "source": [
        "ypred = ypred>0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "D0QQdWCYbZ51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f32665d3-4fa4-4d86-d189-f51131b9ef4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.93      0.96        42\n",
            "           1       0.96      1.00      0.98        72\n",
            "\n",
            "    accuracy                           0.97       114\n",
            "   macro avg       0.98      0.96      0.97       114\n",
            "weighted avg       0.97      0.97      0.97       114\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(ytest, ypred))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ESSOcswicZof"
      },
      "execution_count": 30,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}