{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "loss = (y_pred — y)²/n\n",
        "loss = (y_pred² + y² — 2y*y_pred)/n (expanding the whole square)\n",
        "=>( (x*w+b)² + y² — 2y*(x*w+b))/n (substitute y_pred)\n",
        "=> ((x*w+b)²/n ) + (y²/n) + ((-2y(x*w+b))/n) (splitting the terms)\n",
        "Let A = ((x*w+b)²/n )\n",
        "Let B = (y²/n), \n",
        "Let C = ((-2y(x*w+b))/n)\n",
        "A = ( x²w² + b² + 2xwb )/n (expanding)\n",
        "∂A/∂w = ( 2x²w + 2xb )/n (differentiating)\n",
        "∂B/∂w = 0 (differentiating)\n",
        "C = (-2yxw — 2yb)/n\n",
        "∂C/∂w = (-2yx)/n (differentiating)\n",
        "\n",
        "∂loss/∂w = (2x²w + wxb — 2yx)/n\n",
        "=> (2x(x*w + b — y))/n\n",
        "\n",
        "dw=(2/n)*(y_pred — y)*x\n",
        "db=(2/n)*(y_pred — y)\n",
        "'''"
      ],
      "metadata": {
        "id": "EGWKo2Ox3yJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VeOubHUImT3q"
      },
      "source": [
        "def gradientdescent(x,y):\n",
        "  weight = 0.1\n",
        "  bias = 0.1\n",
        "  learningrate = 0.05\n",
        "  n = len(x)\n",
        "\n",
        "  epochs = 50\n",
        "\n",
        "  for i in range(epochs):  # epochs are iterations\n",
        "    #forward Propagation\n",
        "    ypred = x*weight + bias\n",
        "    mse = mean_squared_error(y, ypred)\n",
        "\n",
        "    #backward Propagation -: step1 of backward propogation\n",
        "    dw = -(2/n)*sum(x*(y-ypred))  # derivative of weight\n",
        "    db = -(2/n)*sum(y-ypred) # derivative of bias\n",
        "\n",
        "    #assign new weights & bias -: step2 of backward propogation\n",
        "    weight = weight - (learningrate*dw) # assigning weights \n",
        "    bias = bias - (learningrate*db) # assigning bias\n",
        "    # where lr is a small number ranging between 0.1 to 0.0000001 (approx).\n",
        "    # Doing this, we can reduce w if slope of tangent of loss at w is positive and increase w if slope is negative.  Learning rate \n",
        "    # is something that we have to manually choose and it is something which we don’t know beforehand. Choosing it is a matter of trial and error.\n",
        "    # The reason we do not directly subtract dw from w is because, it might result in too much change in the value of w and might not end up in \n",
        "    # global minimum but, even further away from it.\n",
        "    print(f\"Iterations -: {i}, bias -: {bias}, weight -: {weight}, MSE -: {mse}\")"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SDQN4m6pkxc"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ouo0uAHapwd5"
      },
      "source": [
        "x = np.array([1,2,3,4])\n",
        "y = np.array([10,12,20,25])"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAttZ2Xjp5ni",
        "outputId": "5dd1e095-af99-4941-8fe6-9686a856b857"
      },
      "source": [
        "gradientdescent(x,y)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iterations -: 0, bias -: 1.74, weight -: 4.85, MSE -: 304.33500000000004\n",
            "Iterations -: 1, bias -: 2.0285, weight -: 5.6275, MSE -: 10.151350000000008\n",
            "Iterations -: 2, bias -: 2.093775, weight -: 5.74975, MSE -: 2.135152874999998\n",
            "Iterations -: 3, bias -: 2.12196, weight -: 5.76399375, MSE -: 1.9072832506249988\n",
            "Iterations -: 4, bias -: 2.1437655625, weight -: 5.7605084374999995, MSE -: 1.8916610056429681\n",
            "Iterations -: 5, bias -: 2.1642618968750003, weight -: 5.75418571875, MSE -: 1.8820949985420456\n",
            "Iterations -: 6, bias -: 2.1842892775000005, weight -: 5.74748095546875, MSE -: 1.8729654313654303\n",
            "Iterations -: 7, bias -: 2.203990110882813, weight -: 5.740797919492187, MSE -: 1.8641112904817663\n",
            "Iterations -: 8, bias -: 2.2233916199214847, weight -: 5.734201952152343, MSE -: 1.855520362583569\n",
            "Iterations -: 9, bias -: 2.2425019698912503, weight -: 5.727702583057715, MSE -: 1.8471847166628241\n",
            "Iterations -: 10, bias -: 2.2613261271376963, weight -: 5.721300153291616, MSE -: 1.8390967640466949\n",
            "Iterations -: 11, bias -: 2.2798684761010226, weight -: 5.71499350653848, MSE -: 1.8312491444622006\n",
            "Iterations -: 12, bias -: 2.2981332518563002, weight -: 5.708781257609364, MSE -: 1.8236347164254376\n",
            "Iterations -: 13, bias -: 2.316124612268329, weight -: 5.702662001438266, MSE -: 1.8162465506634193\n",
            "Iterations -: 14, bias -: 2.33384665068193, weight -: 5.696634347292484, MSE -: 1.8090779238061507\n",
            "Iterations -: 15, bias -: 2.351303398790616, weight -: 5.690696924152638, MSE -: 1.8021223122681795\n",
            "Iterations -: 16, bias -: 2.3684988278733945, weight -: 5.6848483813405055, MSE -: 1.7953733863120034\n",
            "Iterations -: 17, bias -: 2.3854368497509286, weight -: 5.679087388366778, MSE -: 1.7888250042878904\n",
            "Iterations -: 18, bias -: 2.4021213176841414, weight -: 5.673412634653962, MSE -: 1.7824712070448707\n",
            "Iterations -: 19, bias -: 2.4185560272522366, weight -: 5.667822829242455, MSE -: 1.7763062125077818\n",
            "Iterations -: 20, bias -: 2.434744717216399, weight -: 5.662316700497555, MSE -: 1.7703244104154825\n",
            "Iterations -: 21, bias -: 2.4506910703703704, weight -: 5.6568929958202885, MSE -: 1.7645203572154098\n",
            "Iterations -: 22, bias -: 2.4663987143782613, weight -: 5.651550481362479, MSE -: 1.7588887711098378\n",
            "Iterations -: 23, bias -: 2.4818712225998154, weight -: 5.6462879417460545, MSE -: 1.7534245272493438\n",
            "Iterations -: 24, bias -: 2.49711211490332, weight -: 5.64110417978656, MSE -: 1.7481226530691016\n",
            "Iterations -: 25, bias -: 2.5121248584663483, weight -: 5.63599801622081, MSE -: 1.7429783237637517\n",
            "Iterations -: 26, bias -: 2.526912868564511, weight -: 5.6309682894386155, MSE -: 1.7379868578967361\n",
            "Iterations -: 27, bias -: 2.541479509348406, weight -: 5.626013855218526, MSE -: 1.7331437131401064\n",
            "Iterations -: 28, bias -: 2.555828094608934, weight -: 5.62113358646753, MSE -: 1.7284444821409208\n",
            "Iterations -: 29, bias -: 2.569961888531158, weight -: 5.616326372964649, MSE -: 1.7238848885104634\n",
            "Iterations -: 30, bias -: 2.58388410643688, weight -: 5.611591121108373, MSE -: 1.719460782932655\n",
            "Iterations -: 31, bias -: 2.597597915516099, weight -: 5.6069267536678735, MSE -: 1.7151681393880926\n",
            "Iterations -: 32, bias -: 2.6111064355475206, weight -: 5.602332209537944, MSE -: 1.711003051490308\n",
            "Iterations -: 33, bias -: 2.6244127396082826, weight -: 5.597806443497606, MSE -: 1.7069617289308654\n",
            "Iterations -: 34, bias -: 2.637519854773053, weight -: 5.593348425972331, MSE -: 1.7030404940301225\n",
            "Iterations -: 35, bias -: 2.6504307628026647, weight -: 5.58895714279982, MSE -: 1.6992357783904657\n",
            "Iterations -: 36, bias -: 2.6631484008224433, weight -: 5.584631594999289, MSE -: 1.695544119649007\n",
            "Iterations -: 37, bias -: 2.6756756619903768, weight -: 5.5803707985442115, MSE -: 1.6919621583267597\n",
            "Iterations -: 38, bias -: 2.6880153961552864, weight -: 5.576173784138459, MSE -: 1.688486634771463\n",
            "Iterations -: 39, bias -: 2.700170410505143, weight -: 5.572039596995793, MSE -: 1.6851143861912177\n",
            "Iterations -: 40, bias -: 2.7121434702056804, weight -: 5.567967296622663, MSE -: 1.6818423437763048\n",
            "Iterations -: 41, bias -: 2.7239372990294464, weight -: 5.563955956604246, MSE -: 1.6786675299065013\n",
            "Iterations -: 42, bias -: 2.73555457997544, weight -: 5.560004664393699, MSE -: 1.6755870554413923\n",
            "Iterations -: 43, bias -: 2.7469979558794715, weight -: 5.5561125211045646, MSE -: 1.6725981170911974\n",
            "Iterations -: 44, bias -: 2.758270030015383, weight -: 5.552278641306273, MSE -: 1.6696979948657193\n",
            "Iterations -: 45, bias -: 2.7693733666872764, weight -: 5.5485021528227225, MSE -: 1.6668840495991155\n",
            "Iterations -: 46, bias -: 2.7803104918128683, weight -: 5.544782196533862, MSE -: 1.6641537205481944\n",
            "Iterations -: 47, bias -: 2.791083893498116, weight -: 5.541117926180248, MSE -: 1.6615045230620964\n",
            "Iterations -: 48, bias -: 2.8016960226032426, weight -: 5.537508508170533, MSE -: 1.658934046321218\n",
            "Iterations -: 49, bias -: 2.812149293300285, weight -: 5.533953121391822, MSE -: 1.6564399511433057\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqDhmZECjbeL"
      },
      "source": [
        "# the final error that we are receiving is 1.65\n",
        "# Gradient descent is an optimizer. There are many more optimizers we have. Like adam"
      ],
      "execution_count": 4,
      "outputs": []
    }
  ]
}